{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertForSequenceClassification, BertTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import softmax\n",
    "from google.cloud import storage \n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare global variables\n",
    "GCP_KEY = \"/Users/lu31635/Desktop/AC215/ac215.json\"\n",
    "GCP_DATA_BUCKET = 'data-lnt'\n",
    "GCP_SOURCE_FILENAME = 'processed/initial_labels.csv'\n",
    "GCP_MODELS_BUCKET = 'models-lnt'\n",
    "\n",
    "HIGH_CONFIDENCE_THRESHOLD = 0.9\n",
    "TEST_SIZE = 0.2\n",
    "NUMBER_EPOCHS = 3\n",
    "RANDOM_STATE = 215\n",
    "ADAM_LEARNING_RATE = 1e-5\n",
    "ADAM_BATCH_SIZE = 32\n",
    "LABEL_BATCH_SIZE = 64\n",
    "\n",
    "MODEL_DIR_FINETUNE = 'fine_tune_label'\n",
    "MODEL_DIR_BERT = 'bert_label'\n",
    "\n",
    "SENTIMENT_LABELS = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "GCP_OUTPUT_PATH = 'processed/labeled_final.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create GCP Client\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GCP_KEY\n",
    "source_filename = GCP_SOURCE_FILENAME\n",
    "storage_client = storage.Client()\n",
    "data_bucket = storage_client.bucket(GCP_DATA_BUCKET)\n",
    "models_bucket = storage_client.bucket(GCP_MODELS_BUCKET)\n",
    "blob = data_bucket.blob(source_filename)\n",
    "content = blob.download_as_text()\n",
    "#Sanity check\n",
    "print(content[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_directory):\n",
    "    \"\"\"\n",
    "    Input: model_name (name of desired BERT model)\n",
    "    Output: tokenizer, model\n",
    "\n",
    "    >>> get_model(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "    tokenizer(model_name), model(model_name)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_directory)\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_confidence_df(df, threshold=HIGH_CONFIDENCE_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Filters an input dataframe to only include samples with label confidence above a defined threshold\n",
    "\n",
    "    Input: dataframe with initial labels, desired confidence threshold\n",
    "    Output: dataframe including only high confidence examples above specified threshold\n",
    "\n",
    "    >>> get_high_confidence_df(df, 0.9)\n",
    "    high_confidence_df\n",
    "    \"\"\"\n",
    "    return df[(df['negative_score'] > threshold) |\n",
    "    (df['neutral_score'] > threshold) |\n",
    "    (df['positive_score'] > threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    \"\"\"\n",
    "    Returns tokenized text given a dataframe\n",
    "\n",
    "    Input: Pandas dataframe (assumes text column = 'text')\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    text_samples = dataframe['text'].tolist()\n",
    "    tokenized_texts = tokenizer(text_samples, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df, tokenizer, test_size=TEST_SIZE):\n",
    "    \"\"\"\n",
    "    Returns training and validation datasets given a dataframe and tokenizer\n",
    "\n",
    "    Input: panadas dataframe, tokenizer, \n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    # Define training and valid dataframes\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Tokenize the training data\n",
    "    train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    train_labels = torch.tensor(train_df['label'].tolist())\n",
    "\n",
    "    # Tokenize the validation data\n",
    "    valid_encodings = tokenizer(valid_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    valid_labels = torch.tensor(valid_df['label'].tolist())\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_dataset = TensorDataset(train_encodings.input_ids, train_encodings.attention_mask, train_labels)\n",
    "    valid_dataset = TensorDataset(valid_encodings.input_ids, valid_encodings.attention_mask, valid_labels)\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(model, train_dataset, valid_dataset, epochs=NUMBER_EPOCHS):\n",
    "    \"\"\"\n",
    "    Fine tunes the pretrained BERT model based on the high confidence samples\n",
    "\n",
    "    Input: BERT model, training dataset, validation dataset, number of epochs\n",
    "    Output: None (Prints epoch progress)\n",
    "\n",
    "    >>> train_bert(high_confidence_df, train_data, valid_data, epochs=4)\n",
    "    Epoch 2/4: Validation Loss: 12.3452, Validation Accuracy: 0.8362\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define device for compuation\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Train loop\n",
    "    optimizer = AdamW(model.parameters(), lr=ADAM_LEARNING_RATE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=ADAM_BATCH_SIZE, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # Validation loop\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=ADAM_BATCH_SIZE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in valid_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        average_loss = total_loss / len(valid_loader)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}: Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_directory, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Saves the final fine tuned model and tokenizer to GCP models bucket\n",
    "\n",
    "    Input: GCP output directory, model, tokenizer\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = models_bucket.blob(output_directory)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(model, df, batch_size=LABEL_BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Uses the fine-tuned model to evaluate the full initial dataset. New labels are added to the dataframe\n",
    "    based on the label provided by the fine-tuned model.\n",
    "\n",
    "    Input: fine-tuned model, full dataframe\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Define tokenized text\n",
    "    tokenized_texts = tokenize(df)\n",
    "    \n",
    "    #get input IDs and attention mask from tokenized text\n",
    "    input_ids = tokenized_texts['input_ids']\n",
    "    attention_mask = tokenized_texts['attention_mask']\n",
    "    \n",
    "    #define dataset from input IDs and attention mask\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    #define batch size and create DataLoader\n",
    "    batch_size = batch_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #create empty list to store labels for entire dataset\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate through the DataLoader and use the model for labeling\n",
    "    batch = 0\n",
    "    num_batches = -(-len(input_ids) // batch_size)\n",
    "    for batch_input_ids, batch_attention_mask in dataloader:\n",
    "        batch += 1\n",
    "        print(f\"Batch {batch} of {num_batches}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_labels = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        \n",
    "        #append batch labels to dataset label list\n",
    "        labels.append(batch_labels)\n",
    "        \n",
    "    #concatenate all labels\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    #define output logits\n",
    "    logits = labels.logits\n",
    "\n",
    "    #map the predicted labels to sentiment categories\n",
    "    sentiment_labels = SENTIMENT_LABELS\n",
    "    predicted_labels = [sentiment_labels[label] for label in torch.argmax(logits, dim=1).tolist()]\n",
    "\n",
    "    #add the predicted labels to the DataFrame\n",
    "    df['label_fine_tune'] = predicted_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df, outfilepath):\n",
    "    \"\"\"\n",
    "    Saves the labeled dataframe to GCP data bucket\n",
    "    \n",
    "    Input: Pandas dataframe, GCP file path\n",
    "    Output: None\n",
    "\n",
    "    >>> save_dataset(dataframe, 'filepath'):\n",
    "    returns None\n",
    "    \"\"\"\n",
    "\n",
    "    out_file = data_bucket.blob(outfilepath)\n",
    "    df.to_csv(out_file, index=False)\n",
    "    out_file.upload_from_filename(outfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "try: \n",
    "    model_directory = models_bucket.blob(MODEL_DIR_FINETUNE)\n",
    "except:\n",
    "    model_directory = models_bucket.blob(MODEL_DIR_BERT)\n",
    "\n",
    "tokenizer, model = get_model(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import labeled dataset \n",
    "initial_df = pd.read_csv(io.StringIO(content))\n",
    "\n",
    "# Filter high-confidence examples based on predicted sentiment scores\n",
    "high_confidence_df = get_high_confidence_df(initial_df, HIGH_CONFIDENCE_THRESHOLD)\n",
    "high_confidence_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and validation datasets \n",
    "train_data, valid_data = get_datasets(high_confidence_df, tokenizer, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the BERT model\n",
    "train_bert(model, train_data, valid_data, epochs=NUMBER_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model to GCP\n",
    "save_model('fine_tune_label', model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the full dataset using the fine-tuned model\n",
    "label(model, initial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final dataframe to GCP\n",
    "save_dataset(initial_df, GCP_OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
