{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertForSequenceClassification, BertTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import softmax\n",
    "from google.cloud import storage \n",
    "import io\n",
    "import tempfile\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare global variables\n",
    "GCP_KEY = '/home/jupyter/secrets/ac215.json'\n",
    "WANDB_FILE = '/home/jupyter/secrets/wandb.txt'\n",
    "GCP_DATA_BUCKET = 'data-lnt'\n",
    "GCP_MODELS_BUCKET = 'models-lnt'\n",
    "GCP_SOURCE_FILENAME = 'processed/vader_labeled_initial.csv'\n",
    "MODEL_SPECIFICATION = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "OUTPUT_FILEPATH = 'processed/labeled.csv'\n",
    "MODEL_DIR_FINETUNE = 'fine_tune_label'\n",
    "\n",
    "HIGH_CONFIDENCE_THRESHOLD = 0.7\n",
    "TEST_SIZE = 0.2\n",
    "INITIAL_EPOCHS = 1\n",
    "FINAL_EPOCHS = 1\n",
    "RANDOM_STATE = 215\n",
    "ADAM_LEARNING_RATE = 1e-5\n",
    "ADAM_BATCH_SIZE = 32\n",
    "LABEL_BATCH_SIZE = 32\n",
    "PATIENCE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create GCP Client\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GCP_KEY\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(GCP_DATA_BUCKET)\n",
    "source_filename = GCP_SOURCE_FILENAME\n",
    "blob = bucket.blob(source_filename)\n",
    "content = blob.download_as_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # If no GPU is available, use the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukestoner\u001b[0m (\u001b[33mlnt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/AC215_LNT/src/bert_label/wandb/run-20231026_015720-3qpfma2k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lnt/lnt-bert/runs/3qpfma2k' target=\"_blank\">fresh-capybara-6</a></strong> to <a href='https://wandb.ai/lnt/lnt-bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lnt/lnt-bert' target=\"_blank\">https://wandb.ai/lnt/lnt-bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lnt/lnt-bert/runs/3qpfma2k' target=\"_blank\">https://wandb.ai/lnt/lnt-bert/runs/3qpfma2k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lnt/lnt-bert/runs/3qpfma2k?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0bc96d6290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login to weights and biases\n",
    "with open(WANDB_FILE, 'r') as file:\n",
    "    WANDB_KEY = file.read()\n",
    "    \n",
    "wandb.login(key=WANDB_KEY)\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"lnt-bert\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": ADAM_LEARNING_RATE,\n",
    "    \"architecture\": \"BERT\",\n",
    "    \"dataset\": GCP_SOURCE_FILENAME,\n",
    "    \"epochs\": FINAL_EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    \"\"\"\n",
    "    Input: model_name (name of desired BERT model)\n",
    "    Output: tokenizer, model\n",
    "\n",
    "    >>> get_model(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "    tokenizer(model_name), model(model_name)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    \"\"\"\n",
    "    Input: Pandas dataframe (assumes text column = 'text')\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    text_samples = dataframe['text'].tolist()\n",
    "    tokenized_texts = tokenizer(text_samples, padding=True, return_tensors='pt')\n",
    "\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df, labels, tokenizer, test_size=TEST_SIZE):\n",
    "    \"\"\"\n",
    "    Returns training and validation datasets given a dataframe and tokenizer\n",
    "\n",
    "    Input: panadas dataframe, labels column, tokenizer, test size\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    # Define training and valid dataframes\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Tokenize the training data\n",
    "    train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    train_labels = torch.tensor(train_df[labels].tolist())\n",
    "\n",
    "    # Tokenize the validation data\n",
    "    valid_encodings = tokenizer(valid_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    valid_labels = torch.tensor(valid_df[labels].tolist())\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_dataset = TensorDataset(train_encodings.input_ids, train_encodings.attention_mask, train_labels)\n",
    "    valid_dataset = TensorDataset(valid_encodings.input_ids, valid_encodings.attention_mask, valid_labels)\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_initial(model, train_dataset, valid_dataset, device, epochs=INITIAL_EPOCHS, patience=5):\n",
    "    \"\"\"\n",
    "    Fine tunes the pretrained BERT model based on the provided labeled datasets\n",
    "\n",
    "    Input: BERT model, training dataset, validation dataset, number of epochs, patience\n",
    "    Output: None (Prints epoch progress)\n",
    "\n",
    "    >>> train_bert(high_confidence_df, train_data, valid_data, epochs=4, patience=5)\n",
    "    Epoch 2/4: Validation Loss: 12.3452, Validation Accuracy: 0.8362\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    \n",
    "    # Train loop\n",
    "    optimizer = AdamW(model.parameters(), lr=ADAM_LEARNING_RATE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=ADAM_BATCH_SIZE, shuffle=True)\n",
    "    num_batches = round((len(train_dataset) / ADAM_BATCH_SIZE) * .05)\n",
    "    batches_trained = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        #create a progress bar to track labeling process\n",
    "        progress_bar = tqdm(total=len(train_loader), desc=\"Labeling\")\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            batches_trained += 1\n",
    "            \n",
    "            #update progress bar\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            if batches_trained > num_batches:\n",
    "                break\n",
    "\n",
    "        # Validation loop\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=ADAM_BATCH_SIZE)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch in valid_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            accuracy = correct / total\n",
    "            average_loss = total_loss / len(valid_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs}: Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "            # Check for early stopping\n",
    "            if average_loss < best_loss:\n",
    "                best_loss = average_loss\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "\n",
    "            if no_improvement >= patience:\n",
    "                print(f'Early stopping after {epoch + 1} epochs without improvement.')\n",
    "                break  # Stop training\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(tokenized_texts, model, device, dataframe, batch_size=64):\n",
    "    \"\"\"\n",
    "    Uses the BERT model to evaluate the unlabeled dataset. Sentiment scores and labels are added \n",
    "    to the dataframe based on the label provided by the model.\n",
    "\n",
    "    Input: tokenized_texts, model, device, dataframe, batch_size\n",
    "    Output: None\n",
    "    \"\"\"  \n",
    "    #get input IDs and attention mask from tokenized text\n",
    "    input_ids = tokenized_texts['input_ids'].to(device)\n",
    "    attention_mask = tokenized_texts['attention_mask'].to(device)\n",
    "    \n",
    "    #define dataset from input IDs and attention mask\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    #define batch size and create DataLoader\n",
    "    batch_size = batch_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    #set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #create empty list to store labels for entire dataset\n",
    "    labels = []\n",
    "    \n",
    "    #create a progress bar to track labeling process\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Labeling\")\n",
    "    \n",
    "    for batch_input_ids, batch_attention_mask in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        \n",
    "        #get output logits and convert to label confidence\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        #append batch labels to dataset label list\n",
    "        batch_labels = torch.softmax(logits, dim=1)\n",
    "        labels.append(batch_labels)\n",
    "        \n",
    "        #update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    #concatenate all labels\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    #move labels to CPU to append to dataframe\n",
    "    labels = labels.cpu()\n",
    "    \n",
    "    #extract the raw scores for each sentiment class\n",
    "    negative_scores = [score[0].item() for score in labels]\n",
    "    neutral_scores = [score[1].item() for score in labels]\n",
    "    positive_scores = [score[2].item() for score in labels]\n",
    "    \n",
    "    #define final sentiment label my max of sentiment scores\n",
    "    sentiment = []\n",
    "    for neg, neut, pos in zip(negative_scores, neutral_scores, positive_scores):\n",
    "        sentiment.append([neg, neut, pos].index(max([neg, neut, pos])))\n",
    "\n",
    "    #append the scores and predicted labels to the DataFrame\n",
    "    dataframe['negative_score'] = negative_scores\n",
    "    dataframe['neutral_score'] = neutral_scores\n",
    "    dataframe['positive_score'] = positive_scores\n",
    "    dataframe['label'] = sentiment\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_confidence_df(df, threshold=HIGH_CONFIDENCE_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Filters an input dataframe to only include samples with label confidence above a defined threshold\n",
    "\n",
    "    Input: dataframe with initial labels, desired confidence threshold\n",
    "    Output: dataframe including only high confidence examples above specified threshold\n",
    "\n",
    "    >>> get_high_confidence_df(df, 0.9)\n",
    "    high_confidence_df\n",
    "    \"\"\"\n",
    "    return df[(df['negative_score'] > threshold) |\n",
    "    (df['neutral_score'] > threshold) |\n",
    "    (df['positive_score'] > threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final(model, train_dataset, valid_dataset, device, epochs=FINAL_EPOCHS):\n",
    "    \"\"\"\n",
    "    Fine tunes the pretrained BERT model based on the high confidence samples\n",
    "\n",
    "    Input: BERT model, training dataset, validation dataset, number of epochs\n",
    "    Output: None (Prints epoch progress)\n",
    "\n",
    "    >>> train_bert(high_confidence_df, train_data, valid_data, epochs=4)\n",
    "    Epoch 2/4: Validation Loss: 12.3452, Validation Accuracy: 0.8362\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train loop\n",
    "    optimizer = AdamW(model.parameters(), lr=ADAM_LEARNING_RATE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=ADAM_BATCH_SIZE, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        #create a progress bar to track labeling process\n",
    "        progress_bar = tqdm(total=len(train_loader), desc=\"Training\")\n",
    "    \n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #update progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Validation loop\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=ADAM_BATCH_SIZE)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch in valid_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            accuracy = correct / total\n",
    "            average_loss = total_loss / len(valid_loader)\n",
    "            \n",
    "\n",
    "            # log metrics to wandb\n",
    "            wandb.log({\"acc\": accuracy, \"loss\": average_loss})\n",
    "            print(f'Epoch {epoch + 1}/{epochs}: Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df, outfilepath):\n",
    "    \"\"\"\n",
    "    Saves the labeled dataframe to GCP data bucket\n",
    "    \n",
    "    Input: Pandas dataframe, GCP file path\n",
    "    Output: None\n",
    "\n",
    "    >>> save_dataset(dataframe, 'filepath'):\n",
    "    returns None\n",
    "    \"\"\"\n",
    "    #convert DataFrame to a CSV string\n",
    "    csv_string = df.to_csv(index=False)\n",
    "\n",
    "    #upload the CSV string to GCP\n",
    "    blob = bucket.blob(outfilepath)\n",
    "    blob.upload_from_string(csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_directory, models_bucket, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Saves the final fine tuned model and tokenizer to GCP models bucket\n",
    "\n",
    "    Input: GCP output directory, model, tokenizer\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    # Create a temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        \n",
    "        # Serialize and save the model in the temporary directory\n",
    "        model_path = os.path.join(temp_dir, 'model.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Save the tokenizer in the temporary directory\n",
    "        tokenizer.save_pretrained(temp_dir)\n",
    "\n",
    "        # Upload the serialized model to the GCS bucket\n",
    "        bucket = storage_client.bucket(models_bucket)\n",
    "        model_blob = bucket.blob(f'{output_directory}/model.pth')\n",
    "        model_blob.upload_from_filename(model_path)\n",
    "\n",
    "        # Upload the contents of the temporary directory to the GCS bucket\n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                gcs_path = f'{output_directory}/{os.path.relpath(file_path, start=temp_dir)}'\n",
    "                blob = bucket.blob(gcs_path)\n",
    "                blob.upload_from_filename(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>vader_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230622</td>\n",
       "      <td>yesterday i spoke with democrat the presidenti...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 first_name   last_name party   network      date  \\\n",
       "0           0   Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "1           1   Marianne  Williamson     D       FBC  20230622   \n",
       "2           2   Marianne  Williamson     D     CSPAN  20230823   \n",
       "3           3   Marianne  Williamson     D     CSPAN  20230731   \n",
       "4           4   Marianne  Williamson     D     CSPAN  20230813   \n",
       "\n",
       "                                                text  vader_label  \n",
       "0  and . this despite a new poll from rasmussen t...            2  \n",
       "1  yesterday i spoke with democrat the presidenti...            2  \n",
       "2  this time he is doing the same think by senten...            2  \n",
       "3  there is our little friend, her name is . she ...            2  \n",
       "4  and speaking at the des moines register soapbo...            2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import VADER labeled dataset into dataframe\n",
    "VADER_df = pd.read_csv(io.StringIO(content))\n",
    "VADER_df = VADER_df.dropna()\n",
    "#Sanity check\n",
    "VADER_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT model and tokenized text\n",
    "tokenizer, model = get_model(MODEL_SPECIFICATION)\n",
    "tokenized_texts = tokenize(VADER_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "#get training and validation datasets for VADER data\n",
    "train_data_VADER, valid_data_VADER = get_datasets(VADER_df, 'vader_label', tokenizer, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Labeling:   5%|▌         | 55/1072 [02:14<41:33,  2.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: Validation Loss: 0.6868, Validation Accuracy: 0.7117\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fine-tune the BERT model based on VADER labels\n",
    "train_initial(model, train_data_VADER, valid_data_VADER, device, epochs=INITIAL_EPOCHS, patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 1340/1340 [07:09<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "#label the initial dataframe based on newly trained BERT model\n",
    "label(tokenized_texts, model, device, VADER_df, batch_size=LABEL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    27326\n",
       "0    15507\n",
       "1       31\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter high-confidence examples based on predicted sentiment scores\n",
    "VADER_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>vader_label</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.024603</td>\n",
       "      <td>0.021916</td>\n",
       "      <td>0.953481</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.066695</td>\n",
       "      <td>0.027465</td>\n",
       "      <td>0.905841</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.110587</td>\n",
       "      <td>0.044612</td>\n",
       "      <td>0.844801</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030301</td>\n",
       "      <td>0.047786</td>\n",
       "      <td>0.921914</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230616</td>\n",
       "      <td>now, do i believe that almost a quarter of dem...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.092213</td>\n",
       "      <td>0.042785</td>\n",
       "      <td>0.865002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42855</th>\n",
       "      <td>42856</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>MSNBCW</td>\n",
       "      <td>20230717</td>\n",
       "      <td>democratic presidential candidate and anti-vac...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.889344</td>\n",
       "      <td>0.032295</td>\n",
       "      <td>0.078361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42859</th>\n",
       "      <td>42860</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230702</td>\n",
       "      <td>and j . edgar hoover, believe it or j. edgar h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.101442</td>\n",
       "      <td>0.094999</td>\n",
       "      <td>0.803559</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42860</th>\n",
       "      <td>42861</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230702</td>\n",
       "      <td>and j . edgar hoover, believe it or j. edgar h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.101442</td>\n",
       "      <td>0.094999</td>\n",
       "      <td>0.803559</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>42862</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230720</td>\n",
       "      <td>he is more popular -- i hate to say this becau...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.889833</td>\n",
       "      <td>0.023371</td>\n",
       "      <td>0.086796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42862</th>\n",
       "      <td>42863</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230725</td>\n",
       "      <td>how is the biden campaign handling jr.? guest:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092857</td>\n",
       "      <td>0.034624</td>\n",
       "      <td>0.872518</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26535 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 first_name   last_name party   network      date  \\\n",
       "0               0   Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "2               2   Marianne  Williamson     D     CSPAN  20230823   \n",
       "3               3   Marianne  Williamson     D     CSPAN  20230731   \n",
       "4               4   Marianne  Williamson     D     CSPAN  20230813   \n",
       "6               6   Marianne  Williamson     D       FBC  20230616   \n",
       "...           ...        ...         ...   ...       ...       ...   \n",
       "42855       42856     Robert     Kennedy     D    MSNBCW  20230717   \n",
       "42859       42860     Robert     Kennedy     D       GBN  20230702   \n",
       "42860       42861     Robert     Kennedy     D       GBN  20230702   \n",
       "42861       42862     Robert     Kennedy     D     CSPAN  20230720   \n",
       "42862       42863     Robert     Kennedy     D     CSPAN  20230725   \n",
       "\n",
       "                                                    text  vader_label  \\\n",
       "0      and . this despite a new poll from rasmussen t...            2   \n",
       "2      this time he is doing the same think by senten...            2   \n",
       "3      there is our little friend, her name is . she ...            2   \n",
       "4      and speaking at the des moines register soapbo...            2   \n",
       "6      now, do i believe that almost a quarter of dem...            2   \n",
       "...                                                  ...          ...   \n",
       "42855  democratic presidential candidate and anti-vac...            0   \n",
       "42859  and j . edgar hoover, believe it or j. edgar h...            1   \n",
       "42860  and j . edgar hoover, believe it or j. edgar h...            1   \n",
       "42861  he is more popular -- i hate to say this becau...            0   \n",
       "42862  how is the biden campaign handling jr.? guest:...            0   \n",
       "\n",
       "       negative_score  neutral_score  positive_score  label  \n",
       "0            0.024603       0.021916        0.953481      2  \n",
       "2            0.066695       0.027465        0.905841      2  \n",
       "3            0.110587       0.044612        0.844801      2  \n",
       "4            0.030301       0.047786        0.921914      2  \n",
       "6            0.092213       0.042785        0.865002      2  \n",
       "...               ...            ...             ...    ...  \n",
       "42855        0.889344       0.032295        0.078361      0  \n",
       "42859        0.101442       0.094999        0.803559      2  \n",
       "42860        0.101442       0.094999        0.803559      2  \n",
       "42861        0.889833       0.023371        0.086796      0  \n",
       "42862        0.092857       0.034624        0.872518      2  \n",
       "\n",
       "[26535 rows x 12 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence_df = get_high_confidence_df(VADER_df)\n",
    "high_confidence_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    18260\n",
       "0     8275\n",
       "1        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training and validation datasets for BERT data\n",
    "train_data_BERT, valid_data_BERT = get_datasets(high_confidence_df, 'label', tokenizer, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 664/664 [11:02<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: Validation Loss: 0.0106, Validation Accuracy: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fine-tune the BERT model based on VADER labels\n",
    "train_final(model, train_data_BERT, valid_data_BERT, device, epochs=FINAL_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create final dataframe to store scores and labels\n",
    "final_df = VADER_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 1340/1340 [07:09<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "#label the initial dataframe based on newly trained BERT model\n",
    "label(tokenized_texts, model, device, final_df, batch_size=LABEL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>vader_label</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230622</td>\n",
       "      <td>yesterday i spoke with democrat the presidenti...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.999830</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.999791</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42858</th>\n",
       "      <td>42859</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20231010</td>\n",
       "      <td>in america, see this story and i worry at the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998580</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42859</th>\n",
       "      <td>42860</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230702</td>\n",
       "      <td>and j . edgar hoover, believe it or j. edgar h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42860</th>\n",
       "      <td>42861</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230702</td>\n",
       "      <td>and j . edgar hoover, believe it or j. edgar h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>42862</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230720</td>\n",
       "      <td>he is more popular -- i hate to say this becau...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999718</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42862</th>\n",
       "      <td>42863</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230725</td>\n",
       "      <td>how is the biden campaign handling jr.? guest:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.999747</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42863 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 first_name   last_name party   network      date  \\\n",
       "0               0   Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "1               1   Marianne  Williamson     D       FBC  20230622   \n",
       "2               2   Marianne  Williamson     D     CSPAN  20230823   \n",
       "3               3   Marianne  Williamson     D     CSPAN  20230731   \n",
       "4               4   Marianne  Williamson     D     CSPAN  20230813   \n",
       "...           ...        ...         ...   ...       ...       ...   \n",
       "42858       42859     Robert     Kennedy     D       GBN  20231010   \n",
       "42859       42860     Robert     Kennedy     D       GBN  20230702   \n",
       "42860       42861     Robert     Kennedy     D       GBN  20230702   \n",
       "42861       42862     Robert     Kennedy     D     CSPAN  20230720   \n",
       "42862       42863     Robert     Kennedy     D     CSPAN  20230725   \n",
       "\n",
       "                                                    text  vader_label  \\\n",
       "0      and . this despite a new poll from rasmussen t...            2   \n",
       "1      yesterday i spoke with democrat the presidenti...            2   \n",
       "2      this time he is doing the same think by senten...            2   \n",
       "3      there is our little friend, her name is . she ...            2   \n",
       "4      and speaking at the des moines register soapbo...            2   \n",
       "...                                                  ...          ...   \n",
       "42858  in america, see this story and i worry at the ...            2   \n",
       "42859  and j . edgar hoover, believe it or j. edgar h...            1   \n",
       "42860  and j . edgar hoover, believe it or j. edgar h...            1   \n",
       "42861  he is more popular -- i hate to say this becau...            0   \n",
       "42862  how is the biden campaign handling jr.? guest:...            0   \n",
       "\n",
       "       negative_score  neutral_score  positive_score  label  \n",
       "0            0.000072       0.000051        0.999877      2  \n",
       "1            0.000248       0.000018        0.999734      2  \n",
       "2            0.000147       0.000024        0.999830      2  \n",
       "3            0.000183       0.000027        0.999791      2  \n",
       "4            0.000064       0.000067        0.999869      2  \n",
       "...               ...            ...             ...    ...  \n",
       "42858        0.998580       0.000058        0.001363      0  \n",
       "42859        0.000226       0.000067        0.999707      2  \n",
       "42860        0.000226       0.000067        0.999707      2  \n",
       "42861        0.999718       0.000056        0.000227      0  \n",
       "42862        0.000238       0.000016        0.999747      2  \n",
       "\n",
       "[42863 rows x 12 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "final_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    25706\n",
       "0    17156\n",
       "1        2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁█</td></tr><tr><td>loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.99661</td></tr><tr><td>loss</td><td>0.01057</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-capybara-6</strong> at: <a href='https://wandb.ai/lnt/lnt-bert/runs/3qpfma2k' target=\"_blank\">https://wandb.ai/lnt/lnt-bert/runs/3qpfma2k</a><br/> View job at <a href='https://wandb.ai/lnt/lnt-bert/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwOTY3MzkwMQ==/version_details/v1' target=\"_blank\">https://wandb.ai/lnt/lnt-bert/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwOTY3MzkwMQ==/version_details/v1</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_015720-3qpfma2k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#finish weights and biases\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final dataset\n",
    "save_dataset(final_df, OUTPUT_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the fine-tuned model to GCP\n",
    "save_model(MODEL_DIR_FINETUNE, GCP_MODELS_BUCKET, model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
