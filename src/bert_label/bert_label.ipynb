{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertForSequenceClassification, BertTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import softmax\n",
    "from google.cloud import storage \n",
    "import io\n",
    "import tempfile\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare global variables\n",
    "GCP_KEY = '/home/jupyter/secrets/ac215.json'\n",
    "GCP_DATA_BUCKET = 'data-lnt'\n",
    "GCP_MODELS_BUCKET = 'models-lnt'\n",
    "GCP_SOURCE_FILENAME = 'processed/vader_labeled_initial.csv'\n",
    "MODEL_SPECIFICATION = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "OUTPUT_FILEPATH = 'processed/labeled.csv'\n",
    "MODEL_DIR_FINETUNE = 'fine_tune_label'\n",
    "\n",
    "HIGH_CONFIDENCE_THRESHOLD = 0.9\n",
    "TEST_SIZE = 0.2\n",
    "NUMBER_EPOCHS = 1\n",
    "RANDOM_STATE = 215\n",
    "ADAM_LEARNING_RATE = 1e-5\n",
    "ADAM_BATCH_SIZE = 32\n",
    "LABEL_BATCH_SIZE = 32\n",
    "PATIENCE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create GCP Client\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GCP_KEY\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(GCP_DATA_BUCKET)\n",
    "source_filename = GCP_SOURCE_FILENAME\n",
    "blob = bucket.blob(source_filename)\n",
    "content = blob.download_as_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # If no GPU is available, use the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    \"\"\"\n",
    "    Input: model_name (name of desired BERT model)\n",
    "    Output: tokenizer, model\n",
    "\n",
    "    >>> get_model(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "    tokenizer(model_name), model(model_name)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    \"\"\"\n",
    "    Input: Pandas dataframe (assumes text column = 'text')\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    text_samples = dataframe['text'].tolist()\n",
    "    tokenized_texts = tokenizer(text_samples, padding=True, return_tensors='pt')\n",
    "\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df, labels, tokenizer, test_size=TEST_SIZE):\n",
    "    \"\"\"\n",
    "    Returns training and validation datasets given a dataframe and tokenizer\n",
    "\n",
    "    Input: panadas dataframe, labels column, tokenizer, test size\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    # Define training and valid dataframes\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Tokenize the training data\n",
    "    train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    train_labels = torch.tensor(train_df[labels].tolist())\n",
    "\n",
    "    # Tokenize the validation data\n",
    "    valid_encodings = tokenizer(valid_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    valid_labels = torch.tensor(valid_df[labels].tolist())\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_dataset = TensorDataset(train_encodings.input_ids, train_encodings.attention_mask, train_labels)\n",
    "    valid_dataset = TensorDataset(valid_encodings.input_ids, valid_encodings.attention_mask, valid_labels)\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_initial(model, train_dataset, valid_dataset, device, epochs=NUMBER_EPOCHS, patience=5):\n",
    "    \"\"\"\n",
    "    Fine tunes the pretrained BERT model based on the provided labeled datasets\n",
    "\n",
    "    Input: BERT model, training dataset, validation dataset, number of epochs, patience\n",
    "    Output: None (Prints epoch progress)\n",
    "\n",
    "    >>> train_bert(high_confidence_df, train_data, valid_data, epochs=4, patience=5)\n",
    "    Epoch 2/4: Validation Loss: 12.3452, Validation Accuracy: 0.8362\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    \n",
    "    # Train loop\n",
    "    optimizer = AdamW(model.parameters(), lr=ADAM_LEARNING_RATE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=ADAM_BATCH_SIZE, shuffle=True)\n",
    "    num_batches = round((len(train_dataset) / ADAM_BATCH_SIZE) * .25)\n",
    "    batches_trained = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            batches_trained += 1\n",
    "            if batches_trained > num_batches:\n",
    "                break\n",
    "\n",
    "        # Validation loop\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=ADAM_BATCH_SIZE)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch in valid_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            accuracy = correct / total\n",
    "            average_loss = total_loss / len(valid_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs}: Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "            # Check for early stopping\n",
    "            if average_loss < best_loss:\n",
    "                best_loss = average_loss\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "\n",
    "            if no_improvement >= patience:\n",
    "                print(f'Early stopping after {epoch + 1} epochs without improvement.')\n",
    "                break  # Stop training\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(tokenized_texts, model, device, dataframe, batch_size=64):\n",
    "    \"\"\"\n",
    "    Uses the BERT model to evaluate the unlabeled dataset. Sentiment scores and labels are added \n",
    "    to the dataframe based on the label provided by the model.\n",
    "\n",
    "    Input: tokenized_texts, model, device, dataframe, batch_size\n",
    "    Output: None\n",
    "    \"\"\"  \n",
    "    #get input IDs and attention mask from tokenized text\n",
    "    input_ids = tokenized_texts['input_ids'].to(device)\n",
    "    attention_mask = tokenized_texts['attention_mask'].to(device)\n",
    "    \n",
    "    #define dataset from input IDs and attention mask\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    #define batch size and create DataLoader\n",
    "    batch_size = batch_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    #set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #create empty list to store labels for entire dataset\n",
    "    labels = []\n",
    "    \n",
    "    #create a progress bar to track labeling process\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Labeling\")\n",
    "    \n",
    "    for batch_input_ids, batch_attention_mask in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        \n",
    "        #get output logits and convert to label confidence\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        #append batch labels to dataset label list\n",
    "        batch_labels = torch.softmax(logits, dim=1)\n",
    "        labels.append(batch_labels)\n",
    "        \n",
    "        #update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    #concatenate all labels\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    #move labels to CPU to append to dataframe\n",
    "    labels = labels.cpu()\n",
    "    \n",
    "    #extract the raw scores for each sentiment class\n",
    "    negative_scores = [score[0].item() for score in labels]\n",
    "    neutral_scores = [score[1].item() for score in labels]\n",
    "    positive_scores = [score[2].item() for score in labels]\n",
    "    \n",
    "    #define final sentiment label my max of sentiment scores\n",
    "    sentiment = []\n",
    "    for neg, neut, pos in zip(negative_scores, neutral_scores, positive_scores):\n",
    "        sentiment.append([neg, neut, pos].index(max([neg, neut, pos])))\n",
    "\n",
    "    #append the scores and predicted labels to the DataFrame\n",
    "    dataframe['negative_score'] = negative_scores\n",
    "    dataframe['neutral_score'] = neutral_scores\n",
    "    dataframe['positive_score'] = positive_scores\n",
    "    dataframe['label'] = sentiment\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_confidence_df(df, threshold=HIGH_CONFIDENCE_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Filters an input dataframe to only include samples with label confidence above a defined threshold\n",
    "\n",
    "    Input: dataframe with initial labels, desired confidence threshold\n",
    "    Output: dataframe including only high confidence examples above specified threshold\n",
    "\n",
    "    >>> get_high_confidence_df(df, 0.9)\n",
    "    high_confidence_df\n",
    "    \"\"\"\n",
    "    return df[(df['negative_score'] > threshold) |\n",
    "    (df['neutral_score'] > threshold) |\n",
    "    (df['positive_score'] > threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final(model, train_dataset, valid_dataset, device, epochs=NUMBER_EPOCHS):\n",
    "    \"\"\"\n",
    "    Fine tunes the pretrained BERT model based on the high confidence samples\n",
    "\n",
    "    Input: BERT model, training dataset, validation dataset, number of epochs\n",
    "    Output: None (Prints epoch progress)\n",
    "\n",
    "    >>> train_bert(high_confidence_df, train_data, valid_data, epochs=4)\n",
    "    Epoch 2/4: Validation Loss: 12.3452, Validation Accuracy: 0.8362\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train loop\n",
    "    optimizer = AdamW(model.parameters(), lr=ADAM_LEARNING_RATE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=ADAM_BATCH_SIZE, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # Validation loop\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=ADAM_BATCH_SIZE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in valid_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        average_loss = total_loss / len(valid_loader)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}: Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df, outfilepath):\n",
    "    \"\"\"\n",
    "    Saves the labeled dataframe to GCP data bucket\n",
    "    \n",
    "    Input: Pandas dataframe, GCP file path\n",
    "    Output: None\n",
    "\n",
    "    >>> save_dataset(dataframe, 'filepath'):\n",
    "    returns None\n",
    "    \"\"\"\n",
    "    #convert DataFrame to a CSV string\n",
    "    csv_string = df.to_csv(index=False)\n",
    "\n",
    "    #upload the CSV string to GCP\n",
    "    blob = bucket.blob(outfilepath)\n",
    "    blob.upload_from_string(csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_directory, models_bucket, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Saves the final fine tuned model and tokenizer to GCP models bucket\n",
    "\n",
    "    Input: GCP output directory, model, tokenizer\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    # Create a temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        \n",
    "        # Serialize and save the model in the temporary directory\n",
    "        model_path = os.path.join(temp_dir, 'model.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Save the tokenizer in the temporary directory\n",
    "        tokenizer.save_pretrained(temp_dir)\n",
    "\n",
    "        # Upload the serialized model to the GCS bucket\n",
    "        bucket = storage_client.bucket(models_bucket)\n",
    "        model_blob = bucket.blob(f'{output_directory}/model.pth')\n",
    "        model_blob.upload_from_filename(model_path)\n",
    "\n",
    "        # Upload the contents of the temporary directory to the GCS bucket\n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                gcs_path = f'{output_directory}/{os.path.relpath(file_path, start=temp_dir)}'\n",
    "                blob = bucket.blob(gcs_path)\n",
    "                blob.upload_from_filename(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>vader_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230622</td>\n",
       "      <td>yesterday i spoke with democrat the presidenti...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 first_name   last_name party   network      date  \\\n",
       "0           0   Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "1           1   Marianne  Williamson     D       FBC  20230622   \n",
       "2           2   Marianne  Williamson     D     CSPAN  20230823   \n",
       "3           3   Marianne  Williamson     D     CSPAN  20230731   \n",
       "4           4   Marianne  Williamson     D     CSPAN  20230813   \n",
       "\n",
       "                                                text  vader_label  \n",
       "0  and . this despite a new poll from rasmussen t...            2  \n",
       "1  yesterday i spoke with democrat the presidenti...            2  \n",
       "2  this time he is doing the same think by senten...            2  \n",
       "3  there is our little friend, her name is . she ...            2  \n",
       "4  and speaking at the des moines register soapbo...            2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import VADER labeled dataset into dataframe\n",
    "VADER_df = pd.read_csv(io.StringIO(content))\n",
    "VADER_df = VADER_df.dropna()\n",
    "#Sanity check\n",
    "VADER_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT model and tokenized text\n",
    "tokenizer, model = get_model(MODEL_SPECIFICATION)\n",
    "tokenized_texts = tokenize(VADER_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "#get training and validation datasets for VADER data\n",
    "train_data_VADER, valid_data_VADER = get_datasets(VADER_df, 'vader_label', tokenizer, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: Validation Loss: 0.6162, Validation Accuracy: 0.7561\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "#fine-tune the BERT model based on VADER labels\n",
    "train_initial(model, train_data_VADER, valid_data_VADER, device, epochs=NUMBER_EPOCHS, patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 1340/1340 [07:08<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "#label the initial dataframe based on newly trained BERT model\n",
    "label(tokenized_texts, model, device, VADER_df, batch_size=LABEL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    23689\n",
       "0    18399\n",
       "1      776\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter high-confidence examples based on predicted sentiment scores\n",
    "VADER_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>vader_label</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.016312</td>\n",
       "      <td>0.974325</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030915</td>\n",
       "      <td>0.014374</td>\n",
       "      <td>0.954711</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067761</td>\n",
       "      <td>0.027726</td>\n",
       "      <td>0.904513</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.007861</td>\n",
       "      <td>0.031254</td>\n",
       "      <td>0.960884</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230616</td>\n",
       "      <td>now, do i believe that almost a quarter of dem...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.025159</td>\n",
       "      <td>0.024932</td>\n",
       "      <td>0.949908</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42854</th>\n",
       "      <td>42855</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230710</td>\n",
       "      <td>because because junior, be junior, who wants t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028674</td>\n",
       "      <td>0.051736</td>\n",
       "      <td>0.919590</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42855</th>\n",
       "      <td>42856</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>MSNBCW</td>\n",
       "      <td>20230717</td>\n",
       "      <td>democratic presidential candidate and anti-vac...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.942301</td>\n",
       "      <td>0.024905</td>\n",
       "      <td>0.032794</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42858</th>\n",
       "      <td>42859</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20231010</td>\n",
       "      <td>in america, see this story and i worry at the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.094903</td>\n",
       "      <td>0.041354</td>\n",
       "      <td>0.863743</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42859</th>\n",
       "      <td>42860</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230702</td>\n",
       "      <td>and j . edgar hoover, believe it or j. edgar h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047017</td>\n",
       "      <td>0.106098</td>\n",
       "      <td>0.846885</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42860</th>\n",
       "      <td>42861</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230702</td>\n",
       "      <td>and j . edgar hoover, believe it or j. edgar h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047017</td>\n",
       "      <td>0.106098</td>\n",
       "      <td>0.846885</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27149 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 first_name   last_name party   network      date  \\\n",
       "0               0   Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "2               2   Marianne  Williamson     D     CSPAN  20230823   \n",
       "3               3   Marianne  Williamson     D     CSPAN  20230731   \n",
       "4               4   Marianne  Williamson     D     CSPAN  20230813   \n",
       "6               6   Marianne  Williamson     D       FBC  20230616   \n",
       "...           ...        ...         ...   ...       ...       ...   \n",
       "42854       42855     Robert     Kennedy     D       GBN  20230710   \n",
       "42855       42856     Robert     Kennedy     D    MSNBCW  20230717   \n",
       "42858       42859     Robert     Kennedy     D       GBN  20231010   \n",
       "42859       42860     Robert     Kennedy     D       GBN  20230702   \n",
       "42860       42861     Robert     Kennedy     D       GBN  20230702   \n",
       "\n",
       "                                                    text  vader_label  \\\n",
       "0      and . this despite a new poll from rasmussen t...            2   \n",
       "2      this time he is doing the same think by senten...            2   \n",
       "3      there is our little friend, her name is . she ...            2   \n",
       "4      and speaking at the des moines register soapbo...            2   \n",
       "6      now, do i believe that almost a quarter of dem...            2   \n",
       "...                                                  ...          ...   \n",
       "42854  because because junior, be junior, who wants t...            2   \n",
       "42855  democratic presidential candidate and anti-vac...            0   \n",
       "42858  in america, see this story and i worry at the ...            2   \n",
       "42859  and j . edgar hoover, believe it or j. edgar h...            1   \n",
       "42860  and j . edgar hoover, believe it or j. edgar h...            1   \n",
       "\n",
       "       negative_score  neutral_score  positive_score  label  \n",
       "0            0.009363       0.016312        0.974325      2  \n",
       "2            0.030915       0.014374        0.954711      2  \n",
       "3            0.067761       0.027726        0.904513      2  \n",
       "4            0.007861       0.031254        0.960884      2  \n",
       "6            0.025159       0.024932        0.949908      2  \n",
       "...               ...            ...             ...    ...  \n",
       "42854        0.028674       0.051736        0.919590      2  \n",
       "42855        0.942301       0.024905        0.032794      0  \n",
       "42858        0.094903       0.041354        0.863743      2  \n",
       "42859        0.047017       0.106098        0.846885      2  \n",
       "42860        0.047017       0.106098        0.846885      2  \n",
       "\n",
       "[27149 rows x 12 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence_df = get_high_confidence_df(VADER_df)\n",
    "high_confidence_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    15314\n",
       "0    11806\n",
       "1       30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training and validation datasets for BERT data\n",
    "train_data_BERT, valid_data_BERT = get_datasets(high_confidence_df, 'label', tokenizer, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: Validation Loss: 0.0095, Validation Accuracy: 0.9974\n"
     ]
    }
   ],
   "source": [
    "#fine-tune the BERT model based on VADER labels\n",
    "train_final(model, train_data_BERT, valid_data_BERT, device, epochs=NUMBER_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create final dataframe to store scores and labels\n",
    "final_df = VADER_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 1340/1340 [07:07<00:00,  3.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#label the initial dataframe based on newly trained BERT model\n",
    "label(tokenized_texts, model, device, final_df, batch_size=LABEL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>vader_label</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.999716</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230622</td>\n",
       "      <td>yesterday i spoke with democrat the presidenti...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.022803</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.976945</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.999627</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.996270</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.999501</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42858</th>\n",
       "      <td>42859</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20231010</td>\n",
       "      <td>in america, see this story and i worry at the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42859</th>\n",
       "      <td>42860</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230702</td>\n",
       "      <td>and j . edgar hoover, believe it or j. edgar h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.999362</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42860</th>\n",
       "      <td>42861</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230702</td>\n",
       "      <td>and j . edgar hoover, believe it or j. edgar h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.999362</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>42862</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230720</td>\n",
       "      <td>he is more popular -- i hate to say this becau...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999509</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42862</th>\n",
       "      <td>42863</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230725</td>\n",
       "      <td>how is the biden campaign handling jr.? guest:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.817508</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.181729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42863 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 first_name   last_name party   network      date  \\\n",
       "0               0   Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "1               1   Marianne  Williamson     D       FBC  20230622   \n",
       "2               2   Marianne  Williamson     D     CSPAN  20230823   \n",
       "3               3   Marianne  Williamson     D     CSPAN  20230731   \n",
       "4               4   Marianne  Williamson     D     CSPAN  20230813   \n",
       "...           ...        ...         ...   ...       ...       ...   \n",
       "42858       42859     Robert     Kennedy     D       GBN  20231010   \n",
       "42859       42860     Robert     Kennedy     D       GBN  20230702   \n",
       "42860       42861     Robert     Kennedy     D       GBN  20230702   \n",
       "42861       42862     Robert     Kennedy     D     CSPAN  20230720   \n",
       "42862       42863     Robert     Kennedy     D     CSPAN  20230725   \n",
       "\n",
       "                                                    text  vader_label  \\\n",
       "0      and . this despite a new poll from rasmussen t...            2   \n",
       "1      yesterday i spoke with democrat the presidenti...            2   \n",
       "2      this time he is doing the same think by senten...            2   \n",
       "3      there is our little friend, her name is . she ...            2   \n",
       "4      and speaking at the des moines register soapbo...            2   \n",
       "...                                                  ...          ...   \n",
       "42858  in america, see this story and i worry at the ...            2   \n",
       "42859  and j . edgar hoover, believe it or j. edgar h...            1   \n",
       "42860  and j . edgar hoover, believe it or j. edgar h...            1   \n",
       "42861  he is more popular -- i hate to say this becau...            0   \n",
       "42862  how is the biden campaign handling jr.? guest:...            0   \n",
       "\n",
       "       negative_score  neutral_score  positive_score  label  \n",
       "0            0.000124       0.000160        0.999716      2  \n",
       "1            0.022803       0.000252        0.976945      2  \n",
       "2            0.000305       0.000068        0.999627      2  \n",
       "3            0.003566       0.000164        0.996270      2  \n",
       "4            0.000105       0.000394        0.999501      2  \n",
       "...               ...            ...             ...    ...  \n",
       "42858        0.000609       0.000119        0.999272      2  \n",
       "42859        0.000342       0.000296        0.999362      2  \n",
       "42860        0.000342       0.000296        0.999362      2  \n",
       "42861        0.999509       0.000222        0.000269      0  \n",
       "42862        0.817508       0.000763        0.181729      0  \n",
       "\n",
       "[42863 rows x 12 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "final_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    21869\n",
       "0    20603\n",
       "1      392\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final dataset\n",
    "save_dataset(final_df, OUTPUT_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the fine-tuned model to GCP\n",
    "save_model(MODEL_DIR_FINETUNE, GCP_MODELS_BUCKET, model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
