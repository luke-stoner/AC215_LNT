{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b842a46-a121-46d9-ba2e-81d6eef25133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import softmax\n",
    "from google.cloud import storage \n",
    "import io\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c5e17a-e449-460c-9011-b724bb7304a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare global variables\n",
    "GCP_KEY = '/home/jupyter/secrets/ac215.json'\n",
    "GCP_DATA_BUCKET = 'data-lnt'\n",
    "GCP_MODELS_BUCKET = 'models-lnt'\n",
    "GCP_SOURCE_FILENAME = 'raw/unlabeled.csv'\n",
    "MODEL_SPECIFICATION = \"siebert/sentiment-roberta-large-english\"\n",
    "OUTPUT_FILEPATH = 'processed/labeled.csv'\n",
    "MODEL_DIR_FINETUNE = 'fine_tune_label'\n",
    "WANDB_FILE = '/home/jupyter/secrets/wandb.txt'\n",
    "\n",
    "TEST_SIZE = 0.3\n",
    "NUMBER_EPOCHS = 10\n",
    "RANDOM_STATE = 215\n",
    "ADAM_LEARNING_RATE = 1e-5\n",
    "ADAM_BATCH_SIZE = 32\n",
    "LABEL_BATCH_SIZE = 32\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a731c70-cfc4-4a3e-913c-c3ba23f61168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create GCP Client\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GCP_KEY\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(GCP_DATA_BUCKET)\n",
    "source_filename = GCP_SOURCE_FILENAME\n",
    "blob = bucket.blob(source_filename)\n",
    "content = blob.download_as_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ba1b93-e221-467f-ab46-be3ae1c8ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # If no GPU is available, use the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83dc6649-f5d0-4958-9bee-2067c83f89f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukestoner\u001b[0m (\u001b[33mlnt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/AC215_LNT/src/bert_label/wandb/run-20231102_153949-4vyejqqo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lnt/lnt-bert/runs/4vyejqqo' target=\"_blank\">distinctive-fire-9</a></strong> to <a href='https://wandb.ai/lnt/lnt-bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lnt/lnt-bert' target=\"_blank\">https://wandb.ai/lnt/lnt-bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lnt/lnt-bert/runs/4vyejqqo' target=\"_blank\">https://wandb.ai/lnt/lnt-bert/runs/4vyejqqo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lnt/lnt-bert/runs/4vyejqqo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f531d710580>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login to weights and biases\n",
    "with open(WANDB_FILE, 'r') as file:\n",
    "    WANDB_KEY = file.read()\n",
    "    \n",
    "wandb.login(key=WANDB_KEY)\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"lnt-bert\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": ADAM_LEARNING_RATE,\n",
    "    \"architecture\": \"BERT\",\n",
    "    \"dataset\": GCP_SOURCE_FILENAME,\n",
    "    \"epochs\": NUMBER_EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ad8e95-6f36-40fc-968b-3ce73653587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    \"\"\"\n",
    "    Input: model_name (name of desired BERT model)\n",
    "    Output: tokenizer, model\n",
    "\n",
    "    >>> get_model(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "    tokenizer(model_name), model(model_name)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d7868d-af11-4d55-8f7b-606395fcbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    \"\"\"\n",
    "    Input: Pandas dataframe (assumes text column = 'text')\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    text_samples = dataframe['text'].tolist()\n",
    "    tokenized_texts = tokenizer(text_samples, padding=True, return_tensors='pt')\n",
    "\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4210ed1f-7f6e-4e17-b84b-494865dc5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df, labels, tokenizer, test_size=TEST_SIZE):\n",
    "    \"\"\"\n",
    "    Returns training and validation datasets given a dataframe and tokenizer\n",
    "\n",
    "    Input: panadas dataframe, labels column, tokenizer, test size\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    # Define training and valid dataframes\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Tokenize the training data\n",
    "    train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    train_labels = torch.tensor(train_df[labels].tolist())\n",
    "\n",
    "    # Tokenize the validation data\n",
    "    valid_encodings = tokenizer(valid_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    valid_labels = torch.tensor(valid_df[labels].tolist())\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_dataset = TensorDataset(train_encodings.input_ids, train_encodings.attention_mask, train_labels)\n",
    "    valid_dataset = TensorDataset(valid_encodings.input_ids, valid_encodings.attention_mask, valid_labels)\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69abaee-cb00-43e2-a021-b055f4f7176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, valid_dataset, device, epochs=NUMBER_EPOCHS, patience=5):\n",
    "    \"\"\"\n",
    "    Fine tunes the pretrained BERT model based on the provided labeled datasets\n",
    "\n",
    "    Input: BERT model, training dataset, validation dataset, number of epochs, patience\n",
    "    Output: None (Prints epoch progress)\n",
    "\n",
    "    >>> train_bert(high_confidence_df, train_data, valid_data, epochs=4, patience=5)\n",
    "    Epoch 2/4: Validation Loss: 12.3452, Validation Accuracy: 0.8362\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    \n",
    "    # Train loop\n",
    "    optimizer = AdamW(model.parameters(), lr=ADAM_LEARNING_RATE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=ADAM_BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        #create a progress bar to track labeling process\n",
    "        progress_bar = tqdm(total=len(train_loader), desc=\"Labeling\")\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #update progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Validation loop\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=ADAM_BATCH_SIZE)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch in valid_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            accuracy = correct / total\n",
    "            average_loss = total_loss / len(valid_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs}: Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "            # Check for early stopping\n",
    "            if average_loss < best_loss:\n",
    "                best_loss = average_loss\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "\n",
    "            if no_improvement >= patience:\n",
    "                print(f'Early stopping after {epoch + 1} epochs without improvement.')\n",
    "                break  # Stop training\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e564333-37f5-48f1-971a-e7aef8140782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(tokenized_texts, model, device, dataframe, batch_size=64):\n",
    "    \"\"\"\n",
    "    Uses the BERT model to evaluate the unlabeled dataset. Sentiment scores and labels are added \n",
    "    to the dataframe based on the label provided by the model.\n",
    "\n",
    "    Input: tokenized_texts, model, device, dataframe, batch_size\n",
    "    Output: None\n",
    "    \"\"\"  \n",
    "    #get input IDs and attention mask from tokenized text\n",
    "    input_ids = tokenized_texts['input_ids'].to(device)\n",
    "    attention_mask = tokenized_texts['attention_mask'].to(device)\n",
    "    \n",
    "    #define dataset from input IDs and attention mask\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    #define batch size and create DataLoader\n",
    "    batch_size = batch_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    #set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #create empty list to store labels for entire dataset\n",
    "    labels = []\n",
    "    \n",
    "    #create a progress bar to track labeling process\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Labeling\")\n",
    "    \n",
    "    for batch_input_ids, batch_attention_mask in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        \n",
    "        #get output logits and convert to label confidence\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        #append batch labels to dataset label list\n",
    "        batch_labels = torch.softmax(logits, dim=1)\n",
    "        labels.append(batch_labels)\n",
    "        \n",
    "        #update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    #concatenate all labels\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    #move labels to CPU to append to dataframe\n",
    "    labels = labels.cpu()\n",
    "    \n",
    "    #extract the raw scores for each sentiment class\n",
    "    negative_scores = [score[0].item() for score in labels]\n",
    "    positive_scores = [score[1].item() for score in labels]\n",
    "    \n",
    "    #define final sentiment label my max of sentiment scores\n",
    "    sentiment = []\n",
    "    for neg, pos in zip(negative_scores, positive_scores):\n",
    "        sentiment.append([neg, pos].index(max([neg, pos])))\n",
    "\n",
    "    #append the scores and predicted labels to the DataFrame\n",
    "    dataframe['negative_score'] = negative_scores\n",
    "    dataframe['positive_score'] = positive_scores\n",
    "    dataframe['label'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "178316bd-4295-4a66-b4d3-01e332ac8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df, outfilepath):\n",
    "    \"\"\"\n",
    "    Saves the labeled dataframe to GCP data bucket\n",
    "    \n",
    "    Input: Pandas dataframe, GCP file path\n",
    "    Output: None\n",
    "\n",
    "    >>> save_dataset(dataframe, 'filepath'):\n",
    "    returns None\n",
    "    \"\"\"\n",
    "    #convert DataFrame to a CSV string\n",
    "    csv_string = df.to_csv(index=False)\n",
    "\n",
    "    #upload the CSV string to GCP\n",
    "    blob = bucket.blob(outfilepath)\n",
    "    blob.upload_from_string(csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2377d3-62ce-4065-bf07-d4b6ff95e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_directory, models_bucket, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Saves the final fine tuned model and tokenizer to GCP models bucket\n",
    "\n",
    "    Input: GCP output directory, model, tokenizer\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    # Create a temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        \n",
    "        # Serialize and save the model in the temporary directory\n",
    "        model_path = os.path.join(temp_dir, 'model.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Save the tokenizer in the temporary directory\n",
    "        tokenizer.save_pretrained(temp_dir)\n",
    "\n",
    "        # Upload the serialized model to the GCS bucket\n",
    "        bucket = storage_client.bucket(models_bucket)\n",
    "        model_blob = bucket.blob(f'{output_directory}/model.pth')\n",
    "        model_blob.upload_from_filename(model_path)\n",
    "\n",
    "        # Upload the contents of the temporary directory to the GCS bucket\n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                gcs_path = f'{output_directory}/{os.path.relpath(file_path, start=temp_dir)}'\n",
    "                blob = bucket.blob(gcs_path)\n",
    "                blob.upload_from_filename(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa15325-f8cb-4cd4-8da9-da69db6d01b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230622</td>\n",
       "      <td>yesterday i spoke with democrat the presidenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 first_name   last_name party   network      date  \\\n",
       "0           0   Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "1           1   Marianne  Williamson     D       FBC  20230622   \n",
       "2           2   Marianne  Williamson     D     CSPAN  20230823   \n",
       "3           3   Marianne  Williamson     D     CSPAN  20230731   \n",
       "4           4   Marianne  Williamson     D     CSPAN  20230813   \n",
       "\n",
       "                                                text  \n",
       "0  and . this despite a new poll from rasmussen t...  \n",
       "1  yesterday i spoke with democrat the presidenti...  \n",
       "2  this time he is doing the same think by senten...  \n",
       "3  there is our little friend, her name is . she ...  \n",
       "4  and speaking at the des moines register soapbo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import unlabeled dataset into dataframe\n",
    "unlabeled_df = pd.read_csv(io.StringIO(content))\n",
    "unlabeled_df = unlabeled_df.dropna()\n",
    "#Sanity check\n",
    "unlabeled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f731aa7a-d1f9-4eac-a662-f56bde464ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>year</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSNBC</td>\n",
       "      <td>20120126</td>\n",
       "      <td>because in the past any time the democrats hav...</td>\n",
       "      <td>al</td>\n",
       "      <td>gore</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20140304</td>\n",
       "      <td>some people might know of mitch mcconnell nati...</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>McConnell</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FBC</td>\n",
       "      <td>20131113</td>\n",
       "      <td>senator mary landrieu of louisiana, jeff berke...</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Landrieu</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20190627</td>\n",
       "      <td>jersey0-year-old new senator, cory booker, als...</td>\n",
       "      <td>Cory</td>\n",
       "      <td>Booker</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20160211</td>\n",
       "      <td>. the bush surge in new hampshire was the wort...</td>\n",
       "      <td>Marco</td>\n",
       "      <td>Rubio</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    network      date                                               text  \\\n",
       "0     MSNBC  20120126  because in the past any time the democrats hav...   \n",
       "1  FOXNEWSW  20140304  some people might know of mitch mcconnell nati...   \n",
       "2       FBC  20131113  senator mary landrieu of louisiana, jeff berke...   \n",
       "3     CSPAN  20190627  jersey0-year-old new senator, cory booker, als...   \n",
       "4  FOXNEWSW  20160211  . the bush surge in new hampshire was the wort...   \n",
       "\n",
       "   first       last  year  label  \n",
       "0     al       gore  2012      0  \n",
       "1  Mitch  McConnell  2014      0  \n",
       "2   Mary   Landrieu  2014      0  \n",
       "3   Cory     Booker  2020      1  \n",
       "4  Marco      Rubio  2016      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import labeled dataset into dataframe\n",
    "labeled_df = pd.read_csv('labeled_sample.csv')\n",
    "labeled_df = labeled_df.dropna()\n",
    "#Sanity check\n",
    "labeled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c00c1da-de7c-4b82-a7b6-89fddb0a05ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT model and tokenized text for labeled df\n",
    "tokenizer, model = get_model(MODEL_SPECIFICATION)\n",
    "tokenized_texts_labeled = tokenize(labeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "373e692f-d42a-415d-a691-a5e312959dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training and validation datasets for unlabeled data\n",
    "train_data, valid_data = get_datasets(labeled_df, 'label', tokenizer, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d640110c-764b-4be9-8ec8-a04171c57463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Validation Loss: 2.0411, Validation Accuracy: 0.5667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 3/3 [00:06<00:00,  2.10s/it]\n",
      "\n",
      "Labeling:  33%|███▎      | 1/3 [00:02<00:04,  2.16s/it]\u001b[A\n",
      "Labeling:  67%|██████▋   | 2/3 [00:04<00:02,  2.19s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Validation Loss: 1.8796, Validation Accuracy: 0.5667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.90s/it]\n",
      "Labeling: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Validation Loss: 1.6628, Validation Accuracy: 0.5667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.93s/it]\n",
      "\n",
      "Labeling:  33%|███▎      | 1/3 [00:02<00:04,  2.22s/it]\u001b[A\n",
      "Labeling:  67%|██████▋   | 2/3 [00:04<00:02,  2.25s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Validation Loss: 1.4957, Validation Accuracy: 0.5667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]\n",
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Validation Loss: 1.3629, Validation Accuracy: 0.5667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.96s/it]\n",
      "\n",
      "Labeling:  33%|███▎      | 1/3 [00:02<00:04,  2.21s/it]\u001b[A\n",
      "Labeling:  67%|██████▋   | 2/3 [00:04<00:02,  2.23s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: Validation Loss: 1.2743, Validation Accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.93s/it]\n",
      "Labeling: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: Validation Loss: 1.2020, Validation Accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.92s/it]\n",
      "\n",
      "Labeling:  33%|███▎      | 1/3 [00:02<00:04,  2.17s/it]\u001b[A\n",
      "Labeling:  67%|██████▋   | 2/3 [00:04<00:02,  2.20s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: Validation Loss: 1.1524, Validation Accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.90s/it]\n",
      "Labeling: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: Validation Loss: 1.1210, Validation Accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.89s/it]\n",
      "\n",
      "Labeling:  33%|███▎      | 1/3 [00:02<00:04,  2.15s/it]\u001b[A\n",
      "Labeling:  67%|██████▋   | 2/3 [00:04<00:02,  2.17s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 3/3 [00:05<00:00,  1.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: Validation Loss: 1.1083, Validation Accuracy: 0.6000\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fine-tune the BERT model based on labels\n",
    "train(model, train_data, valid_data, device, epochs=NUMBER_EPOCHS, patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed3b642a-7e3f-4554-bfe0-460a7d64b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT model and tokenized text for labeled df\n",
    "tokenized_texts_unlabeled = tokenize(unlabeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c94afd7-3ab9-44b6-a032-248ff8a56b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 1340/1340 [31:57<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "#label the unlabeled dataframe based on newly trained BERT model\n",
    "label(tokenized_texts_unlabeled, model, device, unlabeled_df, batch_size=LABEL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d54dde3-df59-4c1e-9eca-7df79e65bb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    25270\n",
       "1    17594\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf6874c7-852f-4af5-922e-208031cde0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "      <td>0.905626</td>\n",
       "      <td>0.094374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230622</td>\n",
       "      <td>yesterday i spoke with democrat the presidenti...</td>\n",
       "      <td>0.076009</td>\n",
       "      <td>0.923991</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "      <td>0.962190</td>\n",
       "      <td>0.037810</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "      <td>0.064263</td>\n",
       "      <td>0.935737</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "      <td>0.047593</td>\n",
       "      <td>0.952407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42859</th>\n",
       "      <td>42859</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20231010</td>\n",
       "      <td>in america, see this story and i worry at the ...</td>\n",
       "      <td>0.949719</td>\n",
       "      <td>0.050281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42860</th>\n",
       "      <td>42860</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230702</td>\n",
       "      <td>and j . edgar hoover, believe it or j. edgar h...</td>\n",
       "      <td>0.070474</td>\n",
       "      <td>0.929525</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>42861</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>GBN</td>\n",
       "      <td>20230702</td>\n",
       "      <td>and j . edgar hoover, believe it or j. edgar h...</td>\n",
       "      <td>0.070474</td>\n",
       "      <td>0.929525</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42862</th>\n",
       "      <td>42862</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230720</td>\n",
       "      <td>he is more popular -- i hate to say this becau...</td>\n",
       "      <td>0.912827</td>\n",
       "      <td>0.087173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42863</th>\n",
       "      <td>42863</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230725</td>\n",
       "      <td>how is the biden campaign handling jr.? guest:...</td>\n",
       "      <td>0.784404</td>\n",
       "      <td>0.215596</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42863 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 first_name   last_name party   network      date  \\\n",
       "0               0   Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "1               1   Marianne  Williamson     D       FBC  20230622   \n",
       "2               2   Marianne  Williamson     D     CSPAN  20230823   \n",
       "3               3   Marianne  Williamson     D     CSPAN  20230731   \n",
       "4               4   Marianne  Williamson     D     CSPAN  20230813   \n",
       "...           ...        ...         ...   ...       ...       ...   \n",
       "42859       42859     Robert     Kennedy     D       GBN  20231010   \n",
       "42860       42860     Robert     Kennedy     D       GBN  20230702   \n",
       "42861       42861     Robert     Kennedy     D       GBN  20230702   \n",
       "42862       42862     Robert     Kennedy     D     CSPAN  20230720   \n",
       "42863       42863     Robert     Kennedy     D     CSPAN  20230725   \n",
       "\n",
       "                                                    text  negative_score  \\\n",
       "0      and . this despite a new poll from rasmussen t...        0.905626   \n",
       "1      yesterday i spoke with democrat the presidenti...        0.076009   \n",
       "2      this time he is doing the same think by senten...        0.962190   \n",
       "3      there is our little friend, her name is . she ...        0.064263   \n",
       "4      and speaking at the des moines register soapbo...        0.047593   \n",
       "...                                                  ...             ...   \n",
       "42859  in america, see this story and i worry at the ...        0.949719   \n",
       "42860  and j . edgar hoover, believe it or j. edgar h...        0.070474   \n",
       "42861  and j . edgar hoover, believe it or j. edgar h...        0.070474   \n",
       "42862  he is more popular -- i hate to say this becau...        0.912827   \n",
       "42863  how is the biden campaign handling jr.? guest:...        0.784404   \n",
       "\n",
       "       positive_score  label  \n",
       "0            0.094374      0  \n",
       "1            0.923991      1  \n",
       "2            0.037810      0  \n",
       "3            0.935737      1  \n",
       "4            0.952407      1  \n",
       "...               ...    ...  \n",
       "42859        0.050281      0  \n",
       "42860        0.929525      1  \n",
       "42861        0.929525      1  \n",
       "42862        0.087173      0  \n",
       "42863        0.215596      0  \n",
       "\n",
       "[42863 rows x 10 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9c1c262-fbbf-4605-a8f0-b5073f9cdd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-fire-9</strong> at: <a href='https://wandb.ai/lnt/lnt-bert/runs/4vyejqqo' target=\"_blank\">https://wandb.ai/lnt/lnt-bert/runs/4vyejqqo</a><br/> View job at <a href='https://wandb.ai/lnt/lnt-bert/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMjE3MDQwOQ==/version_details/v0' target=\"_blank\">https://wandb.ai/lnt/lnt-bert/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMjE3MDQwOQ==/version_details/v0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231102_153949-4vyejqqo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#finish weights and biases\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccf9c5cb-2df1-4afa-9c03-15a8795f0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final dataset\n",
    "save_dataset(unlabeled_df, OUTPUT_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a11d2f1-0750-4e84-922f-3f36562bab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the fine-tuned model to GCP\n",
    "save_model(MODEL_DIR_FINETUNE, GCP_MODELS_BUCKET, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26738647-953d-4ed5-a14e-1e6ef6aa024a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
