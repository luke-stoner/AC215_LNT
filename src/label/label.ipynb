{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b842a46-a121-46d9-ba2e-81d6eef25133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import softmax\n",
    "from torch.optim import AdamW\n",
    "from google.cloud import storage \n",
    "import io\n",
    "import tempfile\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c5e17a-e449-460c-9011-b724bb7304a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare global variables\n",
    "GCP_KEY = '/home/jupyter/secrets/ac215.json'\n",
    "GCP_DATA_BUCKET = 'data-lnt'\n",
    "GCP_MODELS_BUCKET = 'models-lnt'\n",
    "GCP_SOURCE_FILENAME = 'raw/unlabeled.csv'\n",
    "GCP_HAND_LABEL_FILENAME = 'raw/hand_labeled.csv'\n",
    "MODEL_SPECIFICATION = \"siebert/sentiment-roberta-large-english\"\n",
    "OUTPUT_FILEPATH = 'processed/labeled.csv'\n",
    "MODEL_DIR_FINETUNE = 'fine_tune_label'\n",
    "WANDB_FILE = '/home/jupyter/secrets/wandb.txt'\n",
    "\n",
    "TEST_SIZE = 0.3\n",
    "NUMBER_EPOCHS = 10\n",
    "RANDOM_STATE = 215\n",
    "ADAM_LEARNING_RATE = 2e-5\n",
    "ADAM_BATCH_SIZE = 16\n",
    "LABEL_BATCH_SIZE = 32\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a731c70-cfc4-4a3e-913c-c3ba23f61168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create GCP Client\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GCP_KEY\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(GCP_DATA_BUCKET)\n",
    "unlabeled_blob = bucket.blob(GCP_SOURCE_FILENAME)\n",
    "labeled_blob = bucket.blob(GCP_HAND_LABEL_FILENAME)\n",
    "unlabeled_content = unlabeled_blob.download_as_text()\n",
    "labeled_content = labeled_blob.download_as_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ba1b93-e221-467f-ab46-be3ae1c8ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # If no GPU is available, use the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83dc6649-f5d0-4958-9bee-2067c83f89f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukestoner\u001b[0m (\u001b[33mlnt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/AC215_LNT/src/label/wandb/run-20231107_050329-zg6yjj5g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lnt/lnt-bert/runs/zg6yjj5g' target=\"_blank\">sleek-cosmos-17</a></strong> to <a href='https://wandb.ai/lnt/lnt-bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lnt/lnt-bert' target=\"_blank\">https://wandb.ai/lnt/lnt-bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lnt/lnt-bert/runs/zg6yjj5g' target=\"_blank\">https://wandb.ai/lnt/lnt-bert/runs/zg6yjj5g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lnt/lnt-bert/runs/zg6yjj5g?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f09765a5ba0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login to weights and biases\n",
    "with open(WANDB_FILE, 'r') as file:\n",
    "    WANDB_KEY = file.read()\n",
    "    \n",
    "wandb.login(key=WANDB_KEY)\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"lnt-bert\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": ADAM_LEARNING_RATE,\n",
    "    \"architecture\": \"BERT\",\n",
    "    \"dataset\": GCP_SOURCE_FILENAME,\n",
    "    \"epochs\": NUMBER_EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ad8e95-6f36-40fc-968b-3ce73653587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    \"\"\"\n",
    "    Input: model_name (name of desired BERT model)\n",
    "    Output: tokenizer, model\n",
    "\n",
    "    >>> get_model(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "    tokenizer(model_name), model(model_name)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d7868d-af11-4d55-8f7b-606395fcbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    \"\"\"\n",
    "    Input: Pandas dataframe (assumes text column = 'text')\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    text_samples = dataframe['text'].tolist()\n",
    "    tokenized_texts = tokenizer(text_samples, padding=True, return_tensors='pt')\n",
    "\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4210ed1f-7f6e-4e17-b84b-494865dc5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df, labels, tokenizer, test_size=TEST_SIZE):\n",
    "    \"\"\"\n",
    "    Returns training and validation datasets given a dataframe and tokenizer\n",
    "\n",
    "    Input: panadas dataframe, labels column, tokenizer, test size\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    # Define training and valid dataframes\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Tokenize the training data\n",
    "    train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    train_labels = torch.tensor(train_df[labels].tolist())\n",
    "\n",
    "    # Tokenize the validation data\n",
    "    valid_encodings = tokenizer(valid_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    valid_labels = torch.tensor(valid_df[labels].tolist())\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_dataset = TensorDataset(train_encodings.input_ids, train_encodings.attention_mask, train_labels)\n",
    "    valid_dataset = TensorDataset(valid_encodings.input_ids, valid_encodings.attention_mask, valid_labels)\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69abaee-cb00-43e2-a021-b055f4f7176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, valid_dataset, device, epochs=NUMBER_EPOCHS, patience=5):\n",
    "    \"\"\"\n",
    "    Fine tunes the pretrained BERT model based on the provided labeled datasets\n",
    "\n",
    "    Input: BERT model, training dataset, validation dataset, number of epochs, patience\n",
    "    Output: None (Prints epoch progress)\n",
    "\n",
    "    >>> train_bert(high_confidence_df, train_data, valid_data, epochs=4, patience=5)\n",
    "    Epoch 2/4: Validation Loss: 12.3452, Validation Accuracy: 0.8362\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    \n",
    "    # Train loop\n",
    "    optimizer = AdamW(model.parameters(), lr=ADAM_LEARNING_RATE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=ADAM_BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        #create a progress bar to track labeling process\n",
    "        progress_bar = tqdm(total=len(train_loader), desc=\"Labeling\")\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #update progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Validation loop\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=ADAM_BATCH_SIZE)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch in valid_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            accuracy = correct / total\n",
    "            average_loss = total_loss / len(valid_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs}: Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "            # Check for early stopping\n",
    "            if average_loss < best_loss:\n",
    "                best_loss = average_loss\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "\n",
    "            if no_improvement >= patience:\n",
    "                print(f'Early stopping after {patience} epochs without improvement.')\n",
    "                break  # Stop training\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e564333-37f5-48f1-971a-e7aef8140782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(tokenized_texts, model, device, dataframe, batch_size=64):\n",
    "    \"\"\"\n",
    "    Uses the BERT model to evaluate the unlabeled dataset. Sentiment scores and labels are added \n",
    "    to the dataframe based on the label provided by the model.\n",
    "\n",
    "    Input: tokenized_texts, model, device, dataframe, batch_size\n",
    "    Output: None\n",
    "    \"\"\"  \n",
    "    #get input IDs and attention mask from tokenized text\n",
    "    input_ids = tokenized_texts['input_ids'].to(device)\n",
    "    attention_mask = tokenized_texts['attention_mask'].to(device)\n",
    "    \n",
    "    #define dataset from input IDs and attention mask\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    #define batch size and create DataLoader\n",
    "    batch_size = batch_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    #set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #create empty list to store labels for entire dataset\n",
    "    labels = []\n",
    "    \n",
    "    #create a progress bar to track labeling process\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Labeling\")\n",
    "    \n",
    "    for batch_input_ids, batch_attention_mask in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        \n",
    "        #get output logits and convert to label confidence\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        #append batch labels to dataset label list\n",
    "        batch_labels = torch.softmax(logits, dim=1)\n",
    "        labels.append(batch_labels)\n",
    "        \n",
    "        #update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    #concatenate all labels\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    #move labels to CPU to append to dataframe\n",
    "    labels = labels.cpu()\n",
    "    \n",
    "    #extract the raw scores for each sentiment class\n",
    "    negative_scores = [score[0].item() for score in labels]\n",
    "    positive_scores = [score[1].item() for score in labels]\n",
    "    \n",
    "    #define final sentiment label my max of sentiment scores\n",
    "    sentiment = []\n",
    "    for neg, pos in zip(negative_scores, positive_scores):\n",
    "        sentiment.append([neg, pos].index(max([neg, pos])))\n",
    "\n",
    "    #append the scores and predicted labels to the DataFrame\n",
    "    dataframe['negative_score'] = negative_scores\n",
    "    dataframe['positive_score'] = positive_scores\n",
    "    dataframe['label'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "178316bd-4295-4a66-b4d3-01e332ac8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df, outfilepath):\n",
    "    \"\"\"\n",
    "    Saves the labeled dataframe to GCP data bucket\n",
    "    \n",
    "    Input: Pandas dataframe, GCP file path\n",
    "    Output: None\n",
    "\n",
    "    >>> save_dataset(dataframe, 'filepath'):\n",
    "    returns None\n",
    "    \"\"\"\n",
    "    #convert DataFrame to a CSV string\n",
    "    csv_string = df.to_csv(index=False)\n",
    "\n",
    "    #upload the CSV string to GCP\n",
    "    blob = bucket.blob(outfilepath)\n",
    "    blob.upload_from_string(csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2377d3-62ce-4065-bf07-d4b6ff95e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_directory, models_bucket, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Saves the final fine tuned model and tokenizer to GCP models bucket\n",
    "\n",
    "    Input: GCP output directory, model, tokenizer\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    # Create a temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        \n",
    "        # Serialize and save the model in the temporary directory\n",
    "        model_path = os.path.join(temp_dir, 'model.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Save the tokenizer in the temporary directory\n",
    "        tokenizer.save_pretrained(temp_dir)\n",
    "\n",
    "        # Upload the serialized model to the GCS bucket\n",
    "        bucket = storage_client.bucket(models_bucket)\n",
    "        model_blob = bucket.blob(f'{output_directory}/model.pth')\n",
    "        model_blob.upload_from_filename(model_path)\n",
    "\n",
    "        # Upload the contents of the temporary directory to the GCS bucket\n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                gcs_path = f'{output_directory}/{os.path.relpath(file_path, start=temp_dir)}'\n",
    "                blob = bucket.blob(gcs_path)\n",
    "                blob.upload_from_filename(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa15325-f8cb-4cd4-8da9-da69db6d01b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230622</td>\n",
       "      <td>yesterday i spoke with democrat the presidenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 first_name   last_name party   network      date  \\\n",
       "0           0   Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "1           1   Marianne  Williamson     D       FBC  20230622   \n",
       "2           2   Marianne  Williamson     D     CSPAN  20230823   \n",
       "3           3   Marianne  Williamson     D     CSPAN  20230731   \n",
       "4           4   Marianne  Williamson     D     CSPAN  20230813   \n",
       "\n",
       "                                                text  \n",
       "0  and . this despite a new poll from rasmussen t...  \n",
       "1  yesterday i spoke with democrat the presidenti...  \n",
       "2  this time he is doing the same think by senten...  \n",
       "3  there is our little friend, her name is . she ...  \n",
       "4  and speaking at the des moines register soapbo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import unlabeled dataset into dataframe\n",
    "unlabeled_df = pd.read_csv(io.StringIO(unlabeled_content))\n",
    "unlabeled_df = unlabeled_df.dropna()\n",
    "#Sanity check\n",
    "unlabeled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f731aa7a-d1f9-4eac-a662-f56bde464ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>year</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WUSA</td>\n",
       "      <td>20170914</td>\n",
       "      <td>. florida governor rick scott ordered emergenc...</td>\n",
       "      <td>Rick</td>\n",
       "      <td>Scott</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20130611</td>\n",
       "      <td>a revealing piece of information from republic...</td>\n",
       "      <td>Susan</td>\n",
       "      <td>Collins</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20200120</td>\n",
       "      <td>if mitch mcconnell tries to do what he did to ...</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>McConnell</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20181011</td>\n",
       "      <td>heidi heitkamp, and shes way down in the polls...</td>\n",
       "      <td>Heidi</td>\n",
       "      <td>Heitkamp</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WCVB</td>\n",
       "      <td>20161104</td>\n",
       "      <td>im kelly ayotte.: and when i take the plate fo...</td>\n",
       "      <td>Kelly</td>\n",
       "      <td>Ayotte</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    network      date                                               text  \\\n",
       "0      WUSA  20170914  . florida governor rick scott ordered emergenc...   \n",
       "1  FOXNEWSW  20130611  a revealing piece of information from republic...   \n",
       "2     CSPAN  20200120  if mitch mcconnell tries to do what he did to ...   \n",
       "3  FOXNEWSW  20181011  heidi heitkamp, and shes way down in the polls...   \n",
       "4      WCVB  20161104  im kelly ayotte.: and when i take the plate fo...   \n",
       "\n",
       "   first       last  year  label  \n",
       "0   Rick      Scott  2018      1  \n",
       "1  Susan    Collins  2014      0  \n",
       "2  Mitch  McConnell  2020      0  \n",
       "3  Heidi   Heitkamp  2018      0  \n",
       "4  Kelly     Ayotte  2016      1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import labeled dataset into dataframe\n",
    "labeled_df = pd.read_csv(io.StringIO(labeled_content))\n",
    "labeled_df = labeled_df.dropna()\n",
    "#Sanity check\n",
    "labeled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c00c1da-de7c-4b82-a7b6-89fddb0a05ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT model and tokenized text for labeled df\n",
    "tokenizer, model = get_model(MODEL_SPECIFICATION)\n",
    "tokenized_texts_labeled = tokenize(labeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "373e692f-d42a-415d-a691-a5e312959dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training and validation datasets for unlabeled data\n",
    "train_data, valid_data = get_datasets(labeled_df, 'label', tokenizer, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d640110c-764b-4be9-8ec8-a04171c57463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 9/9 [00:12<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Validation Loss: 0.7721, Validation Accuracy: 0.7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 9/9 [00:14<00:00,  1.65s/it]\n",
      "\n",
      "Labeling:  11%|█         | 1/9 [00:00<00:06,  1.19it/s]\u001b[A\n",
      "Labeling:  22%|██▏       | 2/9 [00:02<00:08,  1.20s/it]\u001b[A\n",
      "Labeling:  33%|███▎      | 3/9 [00:03<00:07,  1.31s/it]\u001b[A\n",
      "Labeling:  44%|████▍     | 4/9 [00:05<00:06,  1.36s/it]\u001b[A\n",
      "Labeling:  56%|█████▌    | 5/9 [00:06<00:05,  1.39s/it]\u001b[A\n",
      "Labeling:  67%|██████▋   | 6/9 [00:08<00:04,  1.40s/it]\u001b[A\n",
      "Labeling:  78%|███████▊  | 7/9 [00:09<00:02,  1.41s/it]\u001b[A\n",
      "Labeling:  89%|████████▉ | 8/9 [00:10<00:01,  1.41s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 9/9 [00:12<00:00,  1.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Validation Loss: 0.6180, Validation Accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 9/9 [00:14<00:00,  1.58s/it]\n",
      "Labeling: 100%|██████████| 9/9 [00:11<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Validation Loss: 0.5578, Validation Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 9/9 [00:13<00:00,  1.53s/it]\n",
      "\n",
      "Labeling:  11%|█         | 1/9 [00:00<00:06,  1.26it/s]\u001b[A\n",
      "Labeling:  22%|██▏       | 2/9 [00:02<00:07,  1.13s/it]\u001b[A\n",
      "Labeling:  33%|███▎      | 3/9 [00:03<00:07,  1.24s/it]\u001b[A\n",
      "Labeling:  44%|████▍     | 4/9 [00:04<00:06,  1.29s/it]\u001b[A\n",
      "Labeling:  56%|█████▌    | 5/9 [00:06<00:05,  1.31s/it]\u001b[A\n",
      "Labeling:  67%|██████▋   | 6/9 [00:07<00:04,  1.33s/it]\u001b[A\n",
      "Labeling:  78%|███████▊  | 7/9 [00:08<00:02,  1.35s/it]\u001b[A\n",
      "Labeling:  89%|████████▉ | 8/9 [00:10<00:01,  1.35s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 9/9 [00:11<00:00,  1.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Validation Loss: 0.6122, Validation Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 9/9 [00:13<00:00,  1.50s/it]\n",
      "Labeling: 100%|██████████| 9/9 [00:11<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Validation Loss: 0.6353, Validation Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 9/9 [00:13<00:00,  1.50s/it]\n",
      "\n",
      "Labeling:  11%|█         | 1/9 [00:00<00:06,  1.27it/s]\u001b[A\n",
      "Labeling:  22%|██▏       | 2/9 [00:02<00:07,  1.13s/it]\u001b[A\n",
      "Labeling:  33%|███▎      | 3/9 [00:03<00:07,  1.24s/it]\u001b[A\n",
      "Labeling:  44%|████▍     | 4/9 [00:04<00:06,  1.29s/it]\u001b[A\n",
      "Labeling:  56%|█████▌    | 5/9 [00:06<00:05,  1.32s/it]\u001b[A\n",
      "Labeling:  67%|██████▋   | 6/9 [00:07<00:04,  1.34s/it]\u001b[A\n",
      "Labeling:  78%|███████▊  | 7/9 [00:09<00:02,  1.35s/it]\u001b[A\n",
      "Labeling:  89%|████████▉ | 8/9 [00:10<00:01,  1.36s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 9/9 [00:11<00:00,  1.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: Validation Loss: 0.7730, Validation Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 9/9 [00:13<00:00,  1.52s/it]\n",
      "Labeling: 100%|██████████| 9/9 [00:11<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: Validation Loss: 0.8936, Validation Accuracy: 0.7333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 9/9 [00:13<00:00,  1.54s/it]\n",
      "\n",
      "Labeling:  11%|█         | 1/9 [00:00<00:06,  1.24it/s]\u001b[A\n",
      "Labeling:  22%|██▏       | 2/9 [00:02<00:08,  1.16s/it]\u001b[A\n",
      "Labeling:  33%|███▎      | 3/9 [00:03<00:07,  1.27s/it]\u001b[A\n",
      "Labeling:  44%|████▍     | 4/9 [00:05<00:06,  1.32s/it]\u001b[A\n",
      "Labeling:  56%|█████▌    | 5/9 [00:06<00:05,  1.35s/it]\u001b[A\n",
      "Labeling:  67%|██████▋   | 6/9 [00:07<00:04,  1.36s/it]\u001b[A\n",
      "Labeling:  78%|███████▊  | 7/9 [00:09<00:02,  1.38s/it]\u001b[A\n",
      "Labeling:  89%|████████▉ | 8/9 [00:10<00:01,  1.38s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 9/9 [00:13<00:00,  1.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: Validation Loss: 0.9299, Validation Accuracy: 0.7167\n",
      "Early stopping after 8 epochs without improvement.\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fine-tune the BERT model based on labels\n",
    "train(model, train_data, valid_data, device, epochs=NUMBER_EPOCHS, patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b642a-7e3f-4554-bfe0-460a7d64b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT model and tokenized text for labeled df\n",
    "tokenized_texts_unlabeled = tokenize(unlabeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94afd7-3ab9-44b6-a032-248ff8a56b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label the unlabeled dataframe based on newly trained BERT model\n",
    "label(tokenized_texts_unlabeled, model, device, unlabeled_df, batch_size=LABEL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54dde3-df59-4c1e-9eca-7df79e65bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6874c7-852f-4af5-922e-208031cde0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1c262-fbbf-4605-a8f0-b5073f9cdd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finish weights and biases\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9c5cb-2df1-4afa-9c03-15a8795f0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final dataset\n",
    "save_dataset(unlabeled_df, OUTPUT_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11d2f1-0750-4e84-922f-3f36562bab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the fine-tuned model to GCP\n",
    "save_model(MODEL_DIR_FINETUNE, GCP_MODELS_BUCKET, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26738647-953d-4ed5-a14e-1e6ef6aa024a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
