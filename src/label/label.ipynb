{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b842a46-a121-46d9-ba2e-81d6eef25133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import softmax\n",
    "from torch.optim import AdamW\n",
    "from google.cloud import storage \n",
    "import io\n",
    "import tempfile\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5e17a-e449-460c-9011-b724bb7304a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare global variables\n",
    "GCP_KEY = '/home/jupyter/secrets/ac215.json'\n",
    "GCP_DATA_BUCKET = 'data-lnt'\n",
    "GCP_MODELS_BUCKET = 'models-lnt'\n",
    "GCP_SOURCE_FILENAME = 'raw/unlabeled.csv'\n",
    "GCP_HAND_LABEL_FILENAME = 'raw/hand_labeled.csv'\n",
    "MODEL_SPECIFICATION = \"siebert/sentiment-roberta-large-english\"\n",
    "OUTPUT_FILEPATH = 'processed/labeled.csv'\n",
    "MODEL_DIR_FINETUNE = 'fine_tune_label'\n",
    "WANDB_FILE = 'secrets/wandb.txt'\n",
    "\n",
    "TEST_SIZE = 0.3\n",
    "NUMBER_EPOCHS = 10\n",
    "RANDOM_STATE = 215\n",
    "ADAM_LEARNING_RATE = 2e-5\n",
    "ADAM_BATCH_SIZE = 16\n",
    "LABEL_BATCH_SIZE = 32\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a731c70-cfc4-4a3e-913c-c3ba23f61168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create GCP Client\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GCP_KEY\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(GCP_DATA_BUCKET)\n",
    "unlabeled_blob = bucket.blob(GCP_SOURCE_FILENAME)\n",
    "labeled_blob = bucket.blob(GCP_HAND_LABEL_FILENAME)\n",
    "wandb_blob = bucket.blob(WANDB_FILE)\n",
    "\n",
    "#get unlabeled and hand labeled datasets\n",
    "unlabeled_content = unlabeled_blob.download_as_text()\n",
    "labeled_content = labeled_blob.download_as_text()\n",
    "\n",
    "#set wandb key\n",
    "wandb_content = wandb_blob.download_as_string()\n",
    "WANDB_KEY = str(wandb_content)[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba1b93-e221-467f-ab46-be3ae1c8ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # If no GPU is available, use the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc6649-f5d0-4958-9bee-2067c83f89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to weights and biases    \n",
    "wandb.login(key=WANDB_KEY)\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"lnt-bert\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": ADAM_LEARNING_RATE,\n",
    "    \"architecture\": \"BERT\",\n",
    "    \"dataset\": GCP_SOURCE_FILENAME,\n",
    "    \"epochs\": NUMBER_EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad8e95-6f36-40fc-968b-3ce73653587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    \"\"\"\n",
    "    Input: model_name (name of desired BERT model)\n",
    "    Output: tokenizer, model\n",
    "\n",
    "    >>> get_model(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "    tokenizer(model_name), model(model_name)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7868d-af11-4d55-8f7b-606395fcbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    \"\"\"\n",
    "    Input: Pandas dataframe (assumes text column = 'text')\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    text_samples = dataframe['text'].tolist()\n",
    "    tokenized_texts = tokenizer(text_samples, padding=True, return_tensors='pt')\n",
    "\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210ed1f-7f6e-4e17-b84b-494865dc5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df, labels, tokenizer, test_size=TEST_SIZE):\n",
    "    \"\"\"\n",
    "    Returns training and validation datasets given a dataframe and tokenizer\n",
    "\n",
    "    Input: panadas dataframe, labels column, tokenizer, test size\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    # Define training and valid dataframes\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Tokenize the training data\n",
    "    train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    train_labels = torch.tensor(train_df[labels].tolist())\n",
    "\n",
    "    # Tokenize the validation data\n",
    "    valid_encodings = tokenizer(valid_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    valid_labels = torch.tensor(valid_df[labels].tolist())\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_dataset = TensorDataset(train_encodings.input_ids, train_encodings.attention_mask, train_labels)\n",
    "    valid_dataset = TensorDataset(valid_encodings.input_ids, valid_encodings.attention_mask, valid_labels)\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69abaee-cb00-43e2-a021-b055f4f7176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, valid_dataset, device, epochs=NUMBER_EPOCHS, patience=5):\n",
    "    \"\"\"\n",
    "    Fine tunes the pretrained BERT model based on the provided labeled datasets\n",
    "\n",
    "    Input: BERT model, training dataset, validation dataset, number of epochs, patience\n",
    "    Output: None (Prints epoch progress)\n",
    "\n",
    "    >>> train_bert(high_confidence_df, train_data, valid_data, epochs=4, patience=5)\n",
    "    Epoch 2/4: Validation Loss: 12.3452, Validation Accuracy: 0.8362\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    \n",
    "    # Train loop\n",
    "    optimizer = AdamW(model.parameters(), lr=ADAM_LEARNING_RATE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=ADAM_BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        #create a progress bar to track labeling process\n",
    "        progress_bar = tqdm(total=len(train_loader), desc=\"Labeling\")\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #update progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Validation loop\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=ADAM_BATCH_SIZE)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch in valid_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            accuracy = correct / total\n",
    "            average_loss = total_loss / len(valid_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs}: Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "            # Check for early stopping\n",
    "            if average_loss < best_loss:\n",
    "                best_loss = average_loss\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "\n",
    "            if no_improvement >= patience:\n",
    "                print(f'Early stopping after {patience} epochs without improvement.')\n",
    "                break  # Stop training\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e564333-37f5-48f1-971a-e7aef8140782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(tokenized_texts, model, device, dataframe, batch_size=64):\n",
    "    \"\"\"\n",
    "    Uses the BERT model to evaluate the unlabeled dataset. Sentiment scores and labels are added \n",
    "    to the dataframe based on the label provided by the model.\n",
    "\n",
    "    Input: tokenized_texts, model, device, dataframe, batch_size\n",
    "    Output: None\n",
    "    \"\"\"  \n",
    "    #get input IDs and attention mask from tokenized text\n",
    "    input_ids = tokenized_texts['input_ids'].to(device)\n",
    "    attention_mask = tokenized_texts['attention_mask'].to(device)\n",
    "    \n",
    "    #define dataset from input IDs and attention mask\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    #define batch size and create DataLoader\n",
    "    batch_size = batch_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    #set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #create empty list to store labels for entire dataset\n",
    "    labels = []\n",
    "    \n",
    "    #create a progress bar to track labeling process\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Labeling\")\n",
    "    \n",
    "    for batch_input_ids, batch_attention_mask in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        \n",
    "        #get output logits and convert to label confidence\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        #append batch labels to dataset label list\n",
    "        batch_labels = torch.softmax(logits, dim=1)\n",
    "        labels.append(batch_labels)\n",
    "        \n",
    "        #update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    #concatenate all labels\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    #move labels to CPU to append to dataframe\n",
    "    labels = labels.cpu()\n",
    "    \n",
    "    #extract the raw scores for each sentiment class\n",
    "    negative_scores = [score[0].item() for score in labels]\n",
    "    positive_scores = [score[1].item() for score in labels]\n",
    "    \n",
    "    #define final sentiment label my max of sentiment scores\n",
    "    sentiment = []\n",
    "    for neg, pos in zip(negative_scores, positive_scores):\n",
    "        sentiment.append([neg, pos].index(max([neg, pos])))\n",
    "\n",
    "    #append the scores and predicted labels to the DataFrame\n",
    "    dataframe['negative_score'] = negative_scores\n",
    "    dataframe['positive_score'] = positive_scores\n",
    "    dataframe['label'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178316bd-4295-4a66-b4d3-01e332ac8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df, outfilepath):\n",
    "    \"\"\"\n",
    "    Saves the labeled dataframe to GCP data bucket\n",
    "    \n",
    "    Input: Pandas dataframe, GCP file path\n",
    "    Output: None\n",
    "\n",
    "    >>> save_dataset(dataframe, 'filepath'):\n",
    "    returns None\n",
    "    \"\"\"\n",
    "    #convert DataFrame to a CSV string\n",
    "    csv_string = df.to_csv(index=False)\n",
    "\n",
    "    #upload the CSV string to GCP\n",
    "    blob = bucket.blob(outfilepath)\n",
    "    blob.upload_from_string(csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2377d3-62ce-4065-bf07-d4b6ff95e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_directory, models_bucket, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Saves the final fine tuned model and tokenizer to GCP models bucket\n",
    "\n",
    "    Input: GCP output directory, model, tokenizer\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    # Create a temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        \n",
    "        # Serialize and save the model in the temporary directory\n",
    "        model_path = os.path.join(temp_dir, 'model.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Save the tokenizer in the temporary directory\n",
    "        tokenizer.save_pretrained(temp_dir)\n",
    "\n",
    "        # Upload the serialized model to the GCS bucket\n",
    "        bucket = storage_client.bucket(models_bucket)\n",
    "        model_blob = bucket.blob(f'{output_directory}/model.pth')\n",
    "        model_blob.upload_from_filename(model_path)\n",
    "\n",
    "        # Upload the contents of the temporary directory to the GCS bucket\n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                gcs_path = f'{output_directory}/{os.path.relpath(file_path, start=temp_dir)}'\n",
    "                blob = bucket.blob(gcs_path)\n",
    "                blob.upload_from_filename(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa15325-f8cb-4cd4-8da9-da69db6d01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import unlabeled dataset into dataframe\n",
    "unlabeled_df = pd.read_csv(io.StringIO(unlabeled_content))\n",
    "unlabeled_df = unlabeled_df.dropna()\n",
    "#Sanity check\n",
    "unlabeled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731aa7a-d1f9-4eac-a662-f56bde464ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import labeled dataset into dataframe\n",
    "labeled_df = pd.read_csv(io.StringIO(labeled_content))\n",
    "labeled_df = labeled_df.dropna()\n",
    "#Sanity check\n",
    "labeled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00c1da-de7c-4b82-a7b6-89fddb0a05ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT model and tokenized text for labeled df\n",
    "tokenizer, model = get_model(MODEL_SPECIFICATION)\n",
    "tokenized_texts_labeled = tokenize(labeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e692f-d42a-415d-a691-a5e312959dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training and validation datasets for unlabeled data\n",
    "train_data, valid_data = get_datasets(labeled_df, 'label', tokenizer, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640110c-764b-4be9-8ec8-a04171c57463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tune the BERT model based on labels\n",
    "train(model, train_data, valid_data, device, epochs=NUMBER_EPOCHS, patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b642a-7e3f-4554-bfe0-460a7d64b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT model and tokenized text for labeled df\n",
    "tokenized_texts_unlabeled = tokenize(unlabeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94afd7-3ab9-44b6-a032-248ff8a56b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label the unlabeled dataframe based on newly trained BERT model\n",
    "label(tokenized_texts_unlabeled, model, device, unlabeled_df, batch_size=LABEL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54dde3-df59-4c1e-9eca-7df79e65bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6874c7-852f-4af5-922e-208031cde0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1c262-fbbf-4605-a8f0-b5073f9cdd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finish weights and biases\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9c5cb-2df1-4afa-9c03-15a8795f0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final dataset\n",
    "save_dataset(unlabeled_df, OUTPUT_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11d2f1-0750-4e84-922f-3f36562bab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the fine-tuned model to GCP\n",
    "save_model(MODEL_DIR_FINETUNE, GCP_MODELS_BUCKET, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26738647-953d-4ed5-a14e-1e6ef6aa024a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
