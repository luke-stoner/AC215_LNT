{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b842a46-a121-46d9-ba2e-81d6eef25133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import softmax\n",
    "from torch.optim import AdamW\n",
    "from google.cloud import storage \n",
    "import io\n",
    "import tempfile\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c5e17a-e449-460c-9011-b724bb7304a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare global variables\n",
    "GCP_KEY = '/home/jupyter/secrets/ac215.json'\n",
    "GCP_DATA_BUCKET = 'data-lnt'\n",
    "GCP_MODELS_BUCKET = 'models-lnt'\n",
    "GCP_SOURCE_FILENAME = 'raw/unlabeled.csv'\n",
    "GCP_HAND_LABEL_FILENAME = 'raw/hand_labeled.csv'\n",
    "MODEL_SPECIFICATION = \"siebert/sentiment-roberta-large-english\"\n",
    "OUTPUT_FILEPATH = 'processed/labeled.csv'\n",
    "MODEL_DIR_FINETUNE = 'fine_tune_label'\n",
    "WANDB_FILE = 'secrets/wandb.txt'\n",
    "\n",
    "TEST_SIZE = 0.25\n",
    "NUMBER_EPOCHS = 10\n",
    "RANDOM_STATE = 215\n",
    "ADAM_LEARNING_RATE = 2e-5\n",
    "ADAM_BATCH_SIZE = 16\n",
    "LABEL_BATCH_SIZE = 32\n",
    "PATIENCE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a731c70-cfc4-4a3e-913c-c3ba23f61168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create GCP Client\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GCP_KEY\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(GCP_DATA_BUCKET)\n",
    "unlabeled_blob = bucket.blob(GCP_SOURCE_FILENAME)\n",
    "labeled_blob = bucket.blob(GCP_HAND_LABEL_FILENAME)\n",
    "\n",
    "#get WANDB file\n",
    "models_bucket = storage_client.bucket(GCP_MODELS_BUCKET)\n",
    "wandb_blob = models_bucket.blob(WANDB_FILE)\n",
    "\n",
    "#get unlabeled and hand labeled datasets\n",
    "unlabeled_content = unlabeled_blob.download_as_text()\n",
    "labeled_content = labeled_blob.download_as_text()\n",
    "\n",
    "#set wandb key\n",
    "wandb_content = wandb_blob.download_as_string()\n",
    "WANDB_KEY = str(wandb_content)[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ba1b93-e221-467f-ab46-be3ae1c8ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # If no GPU is available, use the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83dc6649-f5d0-4958-9bee-2067c83f89f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukestoner\u001b[0m (\u001b[33mlnt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/AC215_LNT/src/label/wandb/run-20231121_012020-acbj7sdf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lnt/lnt-bert/runs/acbj7sdf' target=\"_blank\">cosmic-dew-25</a></strong> to <a href='https://wandb.ai/lnt/lnt-bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lnt/lnt-bert' target=\"_blank\">https://wandb.ai/lnt/lnt-bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lnt/lnt-bert/runs/acbj7sdf' target=\"_blank\">https://wandb.ai/lnt/lnt-bert/runs/acbj7sdf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lnt/lnt-bert/runs/acbj7sdf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fed72ea9c60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login to weights and biases    \n",
    "wandb.login(key=WANDB_KEY)\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"lnt-bert\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": ADAM_LEARNING_RATE,\n",
    "    \"architecture\": \"BERT\",\n",
    "    \"dataset\": GCP_SOURCE_FILENAME,\n",
    "    \"epochs\": NUMBER_EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ad8e95-6f36-40fc-968b-3ce73653587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    \"\"\"\n",
    "    Input: model_name (name of desired BERT model)\n",
    "    Output: tokenizer, model\n",
    "\n",
    "    >>> get_model(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "    tokenizer(model_name), model(model_name)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d7868d-af11-4d55-8f7b-606395fcbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataframe):\n",
    "    \"\"\"\n",
    "    Input: Pandas dataframe (assumes text column = 'text')\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    text_samples = dataframe['text'].tolist()\n",
    "    tokenized_texts = tokenizer(text_samples, padding=True, return_tensors='pt')\n",
    "\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4210ed1f-7f6e-4e17-b84b-494865dc5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df, labels, tokenizer, test_size=TEST_SIZE):\n",
    "    \"\"\"\n",
    "    Returns training and validation datasets given a dataframe and tokenizer\n",
    "\n",
    "    Input: panadas dataframe, labels column, tokenizer, test size\n",
    "    Output: tokenized text\n",
    "\n",
    "    >>> tokenize(df)\n",
    "    tokenized_texts\n",
    "    \"\"\"\n",
    "    # Define training and valid dataframes\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Tokenize the training data\n",
    "    train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    train_labels = torch.tensor(train_df[labels].tolist())\n",
    "\n",
    "    # Tokenize the validation data\n",
    "    valid_encodings = tokenizer(valid_df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
    "    valid_labels = torch.tensor(valid_df[labels].tolist())\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_dataset = TensorDataset(train_encodings.input_ids, train_encodings.attention_mask, train_labels)\n",
    "    valid_dataset = TensorDataset(valid_encodings.input_ids, valid_encodings.attention_mask, valid_labels)\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69abaee-cb00-43e2-a021-b055f4f7176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, valid_dataset, device, epochs=NUMBER_EPOCHS, patience=5):\n",
    "    \"\"\"\n",
    "    Fine tunes the pretrained BERT model based on the provided labeled datasets\n",
    "\n",
    "    Input: BERT model, training dataset, validation dataset, number of epochs, patience\n",
    "    Output: None (Prints epoch progress)\n",
    "\n",
    "    >>> train_bert(high_confidence_df, train_data, valid_data, epochs=4, patience=5)\n",
    "    Epoch 2/4: Validation Loss: 12.3452, Validation Accuracy: 0.8362\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    \n",
    "    # Train loop\n",
    "    optimizer = AdamW(model.parameters(), lr=ADAM_LEARNING_RATE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=ADAM_BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        #create a progress bar to track labeling process\n",
    "        progress_bar = tqdm(total=len(train_loader), desc=\"Labeling\")\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #update progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Validation loop\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=ADAM_BATCH_SIZE)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch in valid_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            accuracy = correct / total\n",
    "            average_loss = total_loss / len(valid_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs}: Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "            # Check for early stopping\n",
    "            if average_loss < best_loss:\n",
    "                best_loss = average_loss\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "\n",
    "            if no_improvement >= patience:\n",
    "                print(f'Early stopping after {patience} epochs without improvement.')\n",
    "                break  # Stop training\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e564333-37f5-48f1-971a-e7aef8140782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(tokenized_texts, model, device, dataframe, batch_size=64):\n",
    "    \"\"\"\n",
    "    Uses the BERT model to evaluate the unlabeled dataset. Sentiment scores and labels are added \n",
    "    to the dataframe based on the label provided by the model.\n",
    "\n",
    "    Input: tokenized_texts, model, device, dataframe, batch_size\n",
    "    Output: None\n",
    "    \"\"\"  \n",
    "    #get input IDs and attention mask from tokenized text\n",
    "    input_ids = tokenized_texts['input_ids'].to(device)\n",
    "    attention_mask = tokenized_texts['attention_mask'].to(device)\n",
    "    \n",
    "    #define dataset from input IDs and attention mask\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    #define batch size and create DataLoader\n",
    "    batch_size = batch_size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    #set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #create empty list to store labels for entire dataset\n",
    "    labels = []\n",
    "    \n",
    "    #create a progress bar to track labeling process\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Labeling\")\n",
    "    \n",
    "    for batch_input_ids, batch_attention_mask in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        \n",
    "        #get output logits and convert to label confidence\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        #append batch labels to dataset label list\n",
    "        batch_labels = torch.softmax(logits, dim=1)\n",
    "        labels.append(batch_labels)\n",
    "        \n",
    "        #update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    #concatenate all labels\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    #move labels to CPU to append to dataframe\n",
    "    labels = labels.cpu()\n",
    "    \n",
    "    #extract the raw scores for each sentiment class\n",
    "    negative_scores = [score[0].item() for score in labels]\n",
    "    positive_scores = [score[1].item() for score in labels]\n",
    "    \n",
    "    #define final sentiment label my max of sentiment scores\n",
    "    sentiment = []\n",
    "    for neg, pos in zip(negative_scores, positive_scores):\n",
    "        sentiment.append([neg, pos].index(max([neg, pos])))\n",
    "\n",
    "    #append the scores and predicted labels to the DataFrame\n",
    "    dataframe['negative_score'] = negative_scores\n",
    "    dataframe['positive_score'] = positive_scores\n",
    "    dataframe['label'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "178316bd-4295-4a66-b4d3-01e332ac8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df, outfilepath):\n",
    "    \"\"\"\n",
    "    Saves the labeled dataframe to GCP data bucket\n",
    "    \n",
    "    Input: Pandas dataframe, GCP file path\n",
    "    Output: None\n",
    "\n",
    "    >>> save_dataset(dataframe, 'filepath'):\n",
    "    returns None\n",
    "    \"\"\"\n",
    "    #convert DataFrame to a CSV string\n",
    "    csv_string = df.to_csv(index=False)\n",
    "\n",
    "    #upload the CSV string to GCP\n",
    "    blob = bucket.blob(outfilepath)\n",
    "    blob.upload_from_string(csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2377d3-62ce-4065-bf07-d4b6ff95e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_directory, models_bucket, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Saves the final fine tuned model and tokenizer to GCP models bucket\n",
    "\n",
    "    Input: GCP output directory, model, tokenizer\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    # Create a temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        \n",
    "        # Serialize and save the model in the temporary directory\n",
    "        model_path = os.path.join(temp_dir, 'model.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Save the tokenizer in the temporary directory\n",
    "        tokenizer.save_pretrained(temp_dir)\n",
    "\n",
    "        # Upload the serialized model to the GCS bucket\n",
    "        bucket = storage_client.bucket(models_bucket)\n",
    "        model_blob = bucket.blob(f'{output_directory}/model.pth')\n",
    "        model_blob.upload_from_filename(model_path)\n",
    "\n",
    "        # Upload the contents of the temporary directory to the GCS bucket\n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                gcs_path = f'{output_directory}/{os.path.relpath(file_path, start=temp_dir)}'\n",
    "                blob = bucket.blob(gcs_path)\n",
    "                blob.upload_from_filename(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa15325-f8cb-4cd4-8da9-da69db6d01b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230622</td>\n",
       "      <td>yesterday i spoke with democrat the presidenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name   last_name party   network      date  \\\n",
       "0   Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "1   Marianne  Williamson     D       FBC  20230622   \n",
       "2   Marianne  Williamson     D     CSPAN  20230823   \n",
       "3   Marianne  Williamson     D     CSPAN  20230731   \n",
       "4   Marianne  Williamson     D     CSPAN  20230813   \n",
       "\n",
       "                                                text  \n",
       "0  and . this despite a new poll from rasmussen t...  \n",
       "1  yesterday i spoke with democrat the presidenti...  \n",
       "2  this time he is doing the same think by senten...  \n",
       "3  there is our little friend, her name is . she ...  \n",
       "4  and speaking at the des moines register soapbo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import unlabeled dataset into dataframe\n",
    "unlabeled_df = pd.read_csv(io.StringIO(unlabeled_content))\n",
    "unlabeled_df = unlabeled_df.dropna()\n",
    "#Sanity check\n",
    "unlabeled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f731aa7a-d1f9-4eac-a662-f56bde464ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>year</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WUSA</td>\n",
       "      <td>20170914</td>\n",
       "      <td>. florida governor rick scott ordered emergenc...</td>\n",
       "      <td>Rick</td>\n",
       "      <td>Scott</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20130611</td>\n",
       "      <td>a revealing piece of information from republic...</td>\n",
       "      <td>Susan</td>\n",
       "      <td>Collins</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20200120</td>\n",
       "      <td>if mitch mcconnell tries to do what he did to ...</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>McConnell</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20181011</td>\n",
       "      <td>heidi heitkamp, and shes way down in the polls...</td>\n",
       "      <td>Heidi</td>\n",
       "      <td>Heitkamp</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WCVB</td>\n",
       "      <td>20161104</td>\n",
       "      <td>im kelly ayotte.: and when i take the plate fo...</td>\n",
       "      <td>Kelly</td>\n",
       "      <td>Ayotte</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    network      date                                               text  \\\n",
       "0      WUSA  20170914  . florida governor rick scott ordered emergenc...   \n",
       "1  FOXNEWSW  20130611  a revealing piece of information from republic...   \n",
       "2     CSPAN  20200120  if mitch mcconnell tries to do what he did to ...   \n",
       "3  FOXNEWSW  20181011  heidi heitkamp, and shes way down in the polls...   \n",
       "4      WCVB  20161104  im kelly ayotte.: and when i take the plate fo...   \n",
       "\n",
       "   first       last  year  label  \n",
       "0   Rick      Scott  2018      1  \n",
       "1  Susan    Collins  2014      0  \n",
       "2  Mitch  McConnell  2020      0  \n",
       "3  Heidi   Heitkamp  2018      0  \n",
       "4  Kelly     Ayotte  2016      1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import labeled dataset into dataframe\n",
    "labeled_df = pd.read_csv(io.StringIO(labeled_content))\n",
    "labeled_df = labeled_df.dropna()\n",
    "#Sanity check\n",
    "labeled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c00c1da-de7c-4b82-a7b6-89fddb0a05ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT model and tokenized text for labeled df\n",
    "tokenizer, model = get_model(MODEL_SPECIFICATION)\n",
    "tokenized_texts_labeled = tokenize(labeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "373e692f-d42a-415d-a691-a5e312959dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training and validation datasets for unlabeled data\n",
    "train_data, valid_data = get_datasets(labeled_df, 'label', tokenizer, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d640110c-764b-4be9-8ec8-a04171c57463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 10/10 [00:14<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Validation Loss: 0.8677, Validation Accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 10/10 [00:15<00:00,  1.60s/it]\n",
      "\n",
      "Labeling:  10%|█         | 1/10 [00:00<00:07,  1.14it/s]\u001b[A\n",
      "Labeling:  20%|██        | 2/10 [00:02<00:09,  1.25s/it]\u001b[A\n",
      "Labeling:  30%|███       | 3/10 [00:03<00:09,  1.36s/it]\u001b[A\n",
      "Labeling:  40%|████      | 4/10 [00:05<00:08,  1.42s/it]\u001b[A\n",
      "Labeling:  50%|█████     | 5/10 [00:06<00:07,  1.45s/it]\u001b[A\n",
      "Labeling:  60%|██████    | 6/10 [00:08<00:05,  1.47s/it]\u001b[A\n",
      "Labeling:  70%|███████   | 7/10 [00:09<00:04,  1.48s/it]\u001b[A\n",
      "Labeling:  80%|████████  | 8/10 [00:11<00:02,  1.48s/it]\u001b[A\n",
      "Labeling:  90%|█████████ | 9/10 [00:12<00:01,  1.48s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 10/10 [00:13<00:00,  1.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Validation Loss: 0.6009, Validation Accuracy: 0.6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 10/10 [00:15<00:00,  1.56s/it]\n",
      "Labeling: 100%|██████████| 10/10 [00:13<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Validation Loss: 0.9070, Validation Accuracy: 0.6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling: 100%|██████████| 10/10 [00:15<00:00,  1.51s/it]\n",
      "\n",
      "Labeling:  10%|█         | 1/10 [00:00<00:07,  1.20it/s]\u001b[A\n",
      "Labeling:  20%|██        | 2/10 [00:02<00:09,  1.18s/it]\u001b[A\n",
      "Labeling:  30%|███       | 3/10 [00:03<00:09,  1.29s/it]\u001b[A\n",
      "Labeling:  40%|████      | 4/10 [00:05<00:08,  1.35s/it]\u001b[A\n",
      "Labeling:  50%|█████     | 5/10 [00:06<00:06,  1.37s/it]\u001b[A\n",
      "Labeling:  60%|██████    | 6/10 [00:07<00:05,  1.39s/it]\u001b[A\n",
      "Labeling:  70%|███████   | 7/10 [00:09<00:04,  1.40s/it]\u001b[A\n",
      "Labeling:  80%|████████  | 8/10 [00:10<00:02,  1.41s/it]\u001b[A\n",
      "Labeling:  90%|█████████ | 9/10 [00:12<00:01,  1.41s/it]\u001b[A\n",
      "Labeling: 100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Validation Loss: 0.6121, Validation Accuracy: 0.7200\n",
      "Early stopping after 2 epochs without improvement.\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fine-tune the BERT model based on labels\n",
    "train(model, train_data, valid_data, device, epochs=NUMBER_EPOCHS, patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed3b642a-7e3f-4554-bfe0-460a7d64b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BERT model and tokenized text for labeled df\n",
    "tokenized_texts_unlabeled = tokenize(unlabeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c94afd7-3ab9-44b6-a032-248ff8a56b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling: 100%|██████████| 1634/1634 [44:48<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "#label the unlabeled dataframe based on newly trained BERT model\n",
    "label(tokenized_texts_unlabeled, model, device, unlabeled_df, batch_size=LABEL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d54dde3-df59-4c1e-9eca-7df79e65bb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    29997\n",
       "1    22277\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf6874c7-852f-4af5-922e-208031cde0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>party</th>\n",
       "      <th>network</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20230611</td>\n",
       "      <td>and . this despite a new poll from rasmussen t...</td>\n",
       "      <td>0.012088</td>\n",
       "      <td>0.987912</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FBC</td>\n",
       "      <td>20230622</td>\n",
       "      <td>yesterday i spoke with democrat the presidenti...</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>0.991321</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230823</td>\n",
       "      <td>this time he is doing the same think by senten...</td>\n",
       "      <td>0.883946</td>\n",
       "      <td>0.116054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230731</td>\n",
       "      <td>there is our little friend, her name is . she ...</td>\n",
       "      <td>0.443024</td>\n",
       "      <td>0.556976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CSPAN</td>\n",
       "      <td>20230813</td>\n",
       "      <td>and speaking at the des moines register soapbo...</td>\n",
       "      <td>0.012266</td>\n",
       "      <td>0.987734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52269</th>\n",
       "      <td>Robert</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>CNBC</td>\n",
       "      <td>20231110</td>\n",
       "      <td>kennedy and assassination of martin luther kin...</td>\n",
       "      <td>0.797930</td>\n",
       "      <td>0.202070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52270</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>20231110</td>\n",
       "      <td>. you better mentioned marianne williamson. i ...</td>\n",
       "      <td>0.973382</td>\n",
       "      <td>0.026618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52271</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>20231111</td>\n",
       "      <td>you got marianne williamson that wants to run....</td>\n",
       "      <td>0.974385</td>\n",
       "      <td>0.025615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52272</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>FOXNEWSW</td>\n",
       "      <td>20231110</td>\n",
       "      <td>marianne williamson and joe manchin. there are...</td>\n",
       "      <td>0.919792</td>\n",
       "      <td>0.080208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52273</th>\n",
       "      <td>Marianne</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>D</td>\n",
       "      <td>CNNW</td>\n",
       "      <td>20231110</td>\n",
       "      <td>and, you better mention marianne williamson, b...</td>\n",
       "      <td>0.935136</td>\n",
       "      <td>0.064864</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52273 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      first_name   last_name party   network      date  \\\n",
       "0       Marianne  Williamson     D  FOXNEWSW  20230611   \n",
       "1       Marianne  Williamson     D       FBC  20230622   \n",
       "2       Marianne  Williamson     D     CSPAN  20230823   \n",
       "3       Marianne  Williamson     D     CSPAN  20230731   \n",
       "4       Marianne  Williamson     D     CSPAN  20230813   \n",
       "...          ...         ...   ...       ...       ...   \n",
       "52269     Robert     Kennedy     D      CNBC  20231110   \n",
       "52270   Marianne  Williamson     D      CNNW  20231110   \n",
       "52271   Marianne  Williamson     D      CNNW  20231111   \n",
       "52272   Marianne  Williamson     D  FOXNEWSW  20231110   \n",
       "52273   Marianne  Williamson     D      CNNW  20231110   \n",
       "\n",
       "                                                    text  negative_score  \\\n",
       "0      and . this despite a new poll from rasmussen t...        0.012088   \n",
       "1      yesterday i spoke with democrat the presidenti...        0.008679   \n",
       "2      this time he is doing the same think by senten...        0.883946   \n",
       "3      there is our little friend, her name is . she ...        0.443024   \n",
       "4      and speaking at the des moines register soapbo...        0.012266   \n",
       "...                                                  ...             ...   \n",
       "52269  kennedy and assassination of martin luther kin...        0.797930   \n",
       "52270  . you better mentioned marianne williamson. i ...        0.973382   \n",
       "52271  you got marianne williamson that wants to run....        0.974385   \n",
       "52272  marianne williamson and joe manchin. there are...        0.919792   \n",
       "52273  and, you better mention marianne williamson, b...        0.935136   \n",
       "\n",
       "       positive_score  label  \n",
       "0            0.987912      1  \n",
       "1            0.991321      1  \n",
       "2            0.116054      0  \n",
       "3            0.556976      1  \n",
       "4            0.987734      1  \n",
       "...               ...    ...  \n",
       "52269        0.202070      0  \n",
       "52270        0.026618      0  \n",
       "52271        0.025615      0  \n",
       "52272        0.080208      0  \n",
       "52273        0.064864      0  \n",
       "\n",
       "[52273 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9c1c262-fbbf-4605-a8f0-b5073f9cdd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-dew-25</strong> at: <a href='https://wandb.ai/lnt/lnt-bert/runs/acbj7sdf' target=\"_blank\">https://wandb.ai/lnt/lnt-bert/runs/acbj7sdf</a><br/> View job at <a href='https://wandb.ai/lnt/lnt-bert/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzI4OTkzMw==/version_details/v2' target=\"_blank\">https://wandb.ai/lnt/lnt-bert/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzI4OTkzMw==/version_details/v2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231121_012020-acbj7sdf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#finish weights and biases\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccf9c5cb-2df1-4afa-9c03-15a8795f0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final dataset\n",
    "save_dataset(unlabeled_df, OUTPUT_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a11d2f1-0750-4e84-922f-3f36562bab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the fine-tuned model to GCP\n",
    "save_model(MODEL_DIR_FINETUNE, GCP_MODELS_BUCKET, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7be28-afeb-4e5f-9413-e02267ca9844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
