{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6d8d79",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "Here, we import all the necessary libraries for our script and initialize the chrome driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c698da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "GCP_KEY = os.environ.get('GCP_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7039b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome options for headless mode\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# Initialize driver in headless mode\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "seven_days_ago = current_datetime - timedelta(days=7)\n",
    "\n",
    "# Get the current date and set start and end date\n",
    "end_date = str(current_datetime.strftime('%Y-%m-%d'))\n",
    "start_date = str(seven_days_ago.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GCP client and set bucket name\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'data-lnt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c0eb3",
   "metadata": {},
   "source": [
    "## Text Cleaning Function\n",
    "This function helps in cleaning and clipping text from the Internet Archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ec7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str, max_words: int=100):\n",
    "    '''\n",
    "    Remove irrelevant characters, candidate name, and clip text at 50 words at the next sentence.\n",
    "    Input:\n",
    "    - text: str = input text from Internet Archive.\n",
    "    - max_words: int = maximum number of words to consider if text contains more than max_words words.\n",
    "    Output:\n",
    "    - str = Cleaned and clipped text.\n",
    "    '''\n",
    "    # remove irrelevant characters\n",
    "    text = text.replace('>', '')\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "    # lowercase text\n",
    "    text = text.lower()\n",
    "    text = re.sub(' +',' ', text)\n",
    "    \n",
    "    # clip text at 50 words to the next complete sentence\n",
    "    words = text.split()\n",
    "    n_words = len(words)\n",
    "    if n_words <= 50:\n",
    "        return text\n",
    "    else:\n",
    "        for i in range(50, min(n_words, max_words)):\n",
    "            punc = bool(re.search(r'[.!?]', words[i]))\n",
    "            if not punc is False:\n",
    "                break\n",
    "        return(' '.join(words[:i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec1072",
   "metadata": {},
   "source": [
    "## Fetching Internet Archive Results\n",
    "The function below gathers results about a candidate from the Internet Archive based on their first and last name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23408b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_results(first_name: str, last_name: str):\n",
    "    '''\n",
    "    Gathers results about candidate from Internet Archive.\n",
    "\n",
    "    Input:\n",
    "    - first_name: str = candidate's first name.\n",
    "    - last_name: str = candidate's last name.\n",
    "    '''\n",
    "    # internet archive url\n",
    "    url = f'https://archive.org/details/tv?q=\"{first_name}+{last_name}\"&and%5B%5D=publicdate%3A%5B{START_DATE}+TO+{END_DATE}%5D&page=1'\n",
    "    # webpage\n",
    "    driver.get(url)\n",
    "    \n",
    "    # list results on page\n",
    "    results = driver.find_elements(By.CLASS_NAME, 'item-ia.hov')\n",
    "    \n",
    "    # scroll to bottom of webpage to prompt infinite scrolling and append further results\n",
    "    at_bottom = 0\n",
    "    len_old_results = 0\n",
    "    while at_bottom < 5: # 5 tries to generate more results to accomate for slow load times\n",
    "        time.sleep(5) # load time for each scroll attempt\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        len_old_results = len(results)\n",
    "        results = driver.find_elements(By.CLASS_NAME, 'item-ia.hov')\n",
    "        if len(results) > len_old_results:\n",
    "            at_bottom = 0\n",
    "        else:\n",
    "            at_bottom += 1\n",
    "    \n",
    "    # drop first result (metadata)\n",
    "    return results[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f1dbc",
   "metadata": {},
   "source": [
    "## Uploading Data to Google Cloud Platform (GCP)\n",
    "This function uploads a given local file to the GCP data bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "999f2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_GCP(df, file_path: str):\n",
    "    '''\n",
    "    Upload local file to GCP data bucket.\n",
    "\n",
    "    Input:\n",
    "    df: pandas dataframe with candidate mentions\n",
    "    file_path: str = path to file to upload.\n",
    "    '''\n",
    "    # load credentials\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = GCP_KEY\n",
    "    \n",
    "    # connect to GCP\n",
    "    bucket_path = 'raw/unlabeled.csv'\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(bucket_path)\n",
    "\n",
    "    # get existing data from unlabeled.csv\n",
    "    content = blob.download_as_text()\n",
    "    unlabeled_df = pd.read_csv(io.StringIO(content))\n",
    "    unlabeled_df = unlabeled_df.dropna()\n",
    "\n",
    "    # concat existing data and new data\n",
    "    combined_df = pd.concat([unlabeled_df, df], ignore_index=True)\n",
    "\n",
    "    # upload CSV to bucket\n",
    "    csv_string = combined_df.to_csv(index=False)\n",
    "    blob.upload_from_string(csv_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab888f",
   "metadata": {},
   "source": [
    "## Scrape Function\n",
    "The main function which scrapes Internet Archive mentions for the candidates provided in the 'candidates.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a2dc680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    '''\n",
    "    Scrape Internet Archive mentions between START_DATE and END_DATE for all candidates in candidates.csv.\n",
    "    \n",
    "    Output:\n",
    "    - pd.DataFrame = data frame of results; each row represents one mention of one candidate.\n",
    "    '''\n",
    "    # read list of candidates\n",
    "    bucket_path = 'raw/candidates.csv'\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(bucket_path)\n",
    "    content = blob.download_as_text()\n",
    "    candidates_df = pd.read_csv(io.StringIO(content), names=['first_name', 'last_name', 'party'])\n",
    "    \n",
    "    # fetch data from Internet Archive\n",
    "    mentions = []\n",
    "    for _, row in candidates_df.iterrows():\n",
    "        # candidate parameters\n",
    "        first_name = row['first_name']\n",
    "        last_name = row['last_name']\n",
    "        party = row['party']\n",
    "\n",
    "        # fetch results from internet archive\n",
    "        results = fetch_results(first_name, last_name)\n",
    "\n",
    "        for result in results:\n",
    "            # extract mention host network\n",
    "            network = result.find_element(By.CLASS_NAME, 'byv').text\n",
    "            \n",
    "            # extract mention date\n",
    "            link = result.get_attribute('data-id')\n",
    "            date = link.split('_')[1]\n",
    "            \n",
    "            # extract mention text\n",
    "            text = result.find_element(By.CLASS_NAME, 'sin-detail').text\n",
    "            text = clean_text(text) \n",
    "            \n",
    "            # append mention to dataframe\n",
    "            mentions.append({\n",
    "                'first_name': first_name,\n",
    "                'last_name': last_name,\n",
    "                'party': party,\n",
    "                'network': network,\n",
    "                'date': date,\n",
    "                'text': text\n",
    "            })\n",
    "    \n",
    "    # format as data frame\n",
    "    mentions_df = pd.DataFrame(mentions)\n",
    "    \n",
    "    # save to disk\n",
    "    outfilepath = 'unlabeled.csv'\n",
    "    mentions_df.to_csv(outfilepath)\n",
    "    \n",
    "    # upload to GCP\n",
    "    upload_to_GCP(outfilepath)\n",
    "    return mentions_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ed9f6",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Here, we set the scraping date range and call the main scrape function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8501871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    START_DATE = start_date\n",
    "    END_DATE = end_date\n",
    "    mentions_df = scrape()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
