{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6d8d79",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "Here, we import all the necessary libraries for our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c698da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os, re, time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c0eb3",
   "metadata": {},
   "source": [
    "## Text Cleaning Function\n",
    "This function helps in cleaning and clipping text from the Internet Archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str, max_words: int=100):\n",
    "    '''\n",
    "    Remove irrelevant characters and clip text at 50 words at the next sentence.\n",
    "    Input:\n",
    "    - text: str = input text from Internet Archive.\n",
    "    - max_words: int = maximum number of words to consider if text contains more than max_words words.\n",
    "    Output:\n",
    "    - str = Cleaned and clipped text.\n",
    "    '''\n",
    "    # remove irrelevant characters\n",
    "    text = text.replace('\\'', '')\n",
    "    text = text.replace('>', '')\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # clip text at 50 words to the next complete sentence\n",
    "    words = text.split()\n",
    "    n_words = len(words)\n",
    "    if n_words <= 50:\n",
    "        return text\n",
    "    else:\n",
    "        for i in range(50, min(n_words, max_words)):\n",
    "            punc = bool(re.search(r'[.!?]', words[i]))\n",
    "            if not punc is False:\n",
    "                break\n",
    "        return(' '.join(words[:i+1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec1072",
   "metadata": {},
   "source": [
    "## Fetching Internet Archive Results\n",
    "The function below gathers results about a candidate from the Internet Archive based on their first and last name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23408b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_results(first_name: str, last_name: str):\n",
    "    '''\n",
    "    Gathers results about candidate from Internet Archive.\n",
    "\n",
    "    Input:\n",
    "    - first_name: str = candidate's first name.\n",
    "    - last_name: str = candidate's last name.\n",
    "    '''\n",
    "    # initialize driver\n",
    "    # options = webdriver.ChromeOptions()\n",
    "    # driver = webdriver.Chrome(options=options)\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # internet archive url\n",
    "    url = f'https://archive.org/details/tv?q=\"{first_name}+{last_name}\"&and%5B%5D=publicdate%3A%5B{START_DATE}+TO+{END_DATE}%5D&page=1'\n",
    "    # webpage\n",
    "    driver.get(url)\n",
    "    \n",
    "    # list results on page\n",
    "    results = driver.find_elements(By.CLASS_NAME, 'item-ia.hov')\n",
    "    \n",
    "    # scroll to bottom of webpage to prompt infinite scrolling and append further results\n",
    "    at_bottom = 0\n",
    "    len_old_results = 0\n",
    "    while at_bottom < 3: # 3 tries to generate more results to accomate for slow load times\n",
    "        time.sleep(5) # load time for each scroll attempt\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        len_old_results = len(results)\n",
    "        results = driver.find_elements(By.CLASS_NAME, 'item-ia.hov')\n",
    "        if len(results) > len_old_results:\n",
    "            at_bottom = 0\n",
    "        else:\n",
    "            at_bottom += 1\n",
    "    \n",
    "    # drop first result (metadata)\n",
    "    return results[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f1dbc",
   "metadata": {},
   "source": [
    "## Uploading Data to Google Cloud Platform (GCP)\n",
    "This function uploads a given local file to the GCP data bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_GCP(file_path: str):\n",
    "    '''\n",
    "    Upload local file to GCP data bucket.\n",
    "\n",
    "    Input:\n",
    "    file_path: str = path to file to upload.\n",
    "    '''\n",
    "    # load credentials\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/secrets/credentials.json\"\n",
    "    \n",
    "    # connect to GCP\n",
    "    bucket_name = 'data'\n",
    "    bucket_path = 'raw/unlabeled.csv'\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # upload CSV to bucket\n",
    "    blob = bucket.blob(bucket_path)\n",
    "    blob.upload_from_filename(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab888f",
   "metadata": {},
   "source": [
    "## Scrape Function\n",
    "The main function which scrapes Internet Archive mentions for the candidates provided in the 'candidates.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2dc680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    '''\n",
    "    Scrape Internet Archive mentions between START_DATE and END_DATE for all candidates in candidates.csv.\n",
    "    \n",
    "    Output:\n",
    "    - pd.DataFrame = data frame of results; each row represents one mention of one candidate.\n",
    "    '''\n",
    "    # read list of candidates\n",
    "    candidates_df = pd.read_csv('candidates.csv', names=['first_name', 'last_name', 'party'])\n",
    "    \n",
    "    # fetch data from Internet Archive\n",
    "    mentions = []\n",
    "    for _, row in candidates_df.iterrows():\n",
    "        # candidate parameters\n",
    "        first_name = row['first_name']\n",
    "        last_name = row['last_name']\n",
    "        party = row['party']\n",
    "\n",
    "        # fetch results from internet archive\n",
    "        results = fetch_results(first_name, last_name)\n",
    "\n",
    "        for result in results:\n",
    "            # extract mention host network\n",
    "            network = result.find_element(By.CLASS_NAME, 'byv').text\n",
    "            \n",
    "            # extract mention date\n",
    "            link = result.get_attribute('data-id')\n",
    "            date = link.split('_')[1]\n",
    "            \n",
    "            # extract mention text\n",
    "            text = result.find_element(By.CLASS_NAME, 'sin-detail').text\n",
    "            text = clean_text(text) \n",
    "            \n",
    "            # append mention to dataframe\n",
    "            mentions.append({\n",
    "                'first_name': first_name,\n",
    "                'last_name': last_name,\n",
    "                'party': party,\n",
    "                'network': network,\n",
    "                'date': date,\n",
    "                'text': text\n",
    "            })\n",
    "    \n",
    "    # format as data frame\n",
    "    mentions_df = pd.DataFrame(mentions)\n",
    "    \n",
    "    # save to disk\n",
    "    outfilepath = 'unlabeled.csv'\n",
    "    mentions_df.to_csv(outfilepath)\n",
    "    \n",
    "    # upload to GCP\n",
    "    upload_to_GCP(outfilepath)\n",
    "    return mentions_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ed9f6",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Here, we set the scraping date range and call the main scrape function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    START_DATE = '2023-09-17'\n",
    "    END_DATE = '2023-09-20'\n",
    "    mentions_df = scrape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d7841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
